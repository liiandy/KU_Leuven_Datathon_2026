{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58e1c457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ec9df",
   "metadata": {},
   "source": [
    "## Load CSV data (Dataset A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a184340",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r'C:\\Users\\lxa0530\\OneDrive\\Desktop\\Datathon\\Adding_B_feature\\input\\Spaced Repetition Data.csv'\n",
    "# Change it to your actual path of dataset A\n",
    "\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d51adc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_recall</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>delta</th>\n",
       "      <th>user_id</th>\n",
       "      <th>learning_language</th>\n",
       "      <th>ui_language</th>\n",
       "      <th>lexeme_id</th>\n",
       "      <th>lexeme_string</th>\n",
       "      <th>history_seen</th>\n",
       "      <th>history_correct</th>\n",
       "      <th>session_seen</th>\n",
       "      <th>session_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1362076081</td>\n",
       "      <td>27649635</td>\n",
       "      <td>u:FO</td>\n",
       "      <td>de</td>\n",
       "      <td>en</td>\n",
       "      <td>76390c1350a8dac31186187e2fe1e178</td>\n",
       "      <td>lernt/lernen&lt;vblex&gt;&lt;pri&gt;&lt;p3&gt;&lt;sg&gt;</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1362076081</td>\n",
       "      <td>27649635</td>\n",
       "      <td>u:FO</td>\n",
       "      <td>de</td>\n",
       "      <td>en</td>\n",
       "      <td>7dfd7086f3671685e2cf1c1da72796d7</td>\n",
       "      <td>die/die&lt;det&gt;&lt;def&gt;&lt;f&gt;&lt;sg&gt;&lt;nom&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1362076081</td>\n",
       "      <td>27649635</td>\n",
       "      <td>u:FO</td>\n",
       "      <td>de</td>\n",
       "      <td>en</td>\n",
       "      <td>35a54c25a2cda8127343f6a82e6f6b7d</td>\n",
       "      <td>mann/mann&lt;n&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1362076081</td>\n",
       "      <td>27649635</td>\n",
       "      <td>u:FO</td>\n",
       "      <td>de</td>\n",
       "      <td>en</td>\n",
       "      <td>0cf63ffe3dda158bc3dbd55682b355ae</td>\n",
       "      <td>frau/frau&lt;n&gt;&lt;f&gt;&lt;sg&gt;&lt;nom&gt;</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1362076081</td>\n",
       "      <td>27649635</td>\n",
       "      <td>u:FO</td>\n",
       "      <td>de</td>\n",
       "      <td>en</td>\n",
       "      <td>84920990d78044db53c1b012f5bf9ab5</td>\n",
       "      <td>das/das&lt;det&gt;&lt;def&gt;&lt;nt&gt;&lt;sg&gt;&lt;nom&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_recall   timestamp     delta user_id learning_language ui_language  \\\n",
       "0       1.0  1362076081  27649635    u:FO                de          en   \n",
       "1       0.5  1362076081  27649635    u:FO                de          en   \n",
       "2       1.0  1362076081  27649635    u:FO                de          en   \n",
       "3       0.5  1362076081  27649635    u:FO                de          en   \n",
       "4       1.0  1362076081  27649635    u:FO                de          en   \n",
       "\n",
       "                          lexeme_id                     lexeme_string  \\\n",
       "0  76390c1350a8dac31186187e2fe1e178  lernt/lernen<vblex><pri><p3><sg>   \n",
       "1  7dfd7086f3671685e2cf1c1da72796d7     die/die<det><def><f><sg><nom>   \n",
       "2  35a54c25a2cda8127343f6a82e6f6b7d          mann/mann<n><m><sg><nom>   \n",
       "3  0cf63ffe3dda158bc3dbd55682b355ae          frau/frau<n><f><sg><nom>   \n",
       "4  84920990d78044db53c1b012f5bf9ab5    das/das<det><def><nt><sg><nom>   \n",
       "\n",
       "   history_seen  history_correct  session_seen  session_correct  \n",
       "0             6                4             2                2  \n",
       "1             4                4             2                1  \n",
       "2             5                4             1                1  \n",
       "3             6                5             2                1  \n",
       "4             4                4             1                1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd998334",
   "metadata": {},
   "source": [
    "## Extract the pt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11cc801e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_recall</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>delta</th>\n",
       "      <th>user_id</th>\n",
       "      <th>learning_language</th>\n",
       "      <th>ui_language</th>\n",
       "      <th>lexeme_id</th>\n",
       "      <th>lexeme_string</th>\n",
       "      <th>history_seen</th>\n",
       "      <th>history_correct</th>\n",
       "      <th>session_seen</th>\n",
       "      <th>session_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1362082493</td>\n",
       "      <td>1469</td>\n",
       "      <td>u:g3WM</td>\n",
       "      <td>pt</td>\n",
       "      <td>en</td>\n",
       "      <td>57408f89412af98111a2f87c0ab41b22</td>\n",
       "      <td>tu/tu&lt;prn&gt;&lt;tn&gt;&lt;p2&gt;&lt;mf&gt;&lt;sg&gt;</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1362082493</td>\n",
       "      <td>1469</td>\n",
       "      <td>u:g3WM</td>\n",
       "      <td>pt</td>\n",
       "      <td>en</td>\n",
       "      <td>8414835cb39e4315146a59fefdd6d1c6</td>\n",
       "      <td>tem/ter&lt;vblex&gt;&lt;pri&gt;&lt;p3&gt;&lt;sg&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1362082493</td>\n",
       "      <td>1469</td>\n",
       "      <td>u:g3WM</td>\n",
       "      <td>pt</td>\n",
       "      <td>en</td>\n",
       "      <td>ecc3feb8e53ce936cef181dd54e7aaca</td>\n",
       "      <td>temos/ter&lt;vblex&gt;&lt;pri&gt;&lt;p1&gt;&lt;pl&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1362082493</td>\n",
       "      <td>2184</td>\n",
       "      <td>u:g3WM</td>\n",
       "      <td>pt</td>\n",
       "      <td>en</td>\n",
       "      <td>8d28ba0fa188f1847571467189846dda</td>\n",
       "      <td>tua/teu&lt;det&gt;&lt;pos&gt;&lt;f&gt;&lt;sg&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1362082493</td>\n",
       "      <td>1469</td>\n",
       "      <td>u:g3WM</td>\n",
       "      <td>pt</td>\n",
       "      <td>en</td>\n",
       "      <td>4b3613233b3fede2e3e92ac2ef752bf6</td>\n",
       "      <td>leão/leão&lt;n&gt;&lt;m&gt;&lt;sg&gt;</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12852940</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1363104812</td>\n",
       "      <td>604744</td>\n",
       "      <td>u:h3b5</td>\n",
       "      <td>pt</td>\n",
       "      <td>en</td>\n",
       "      <td>09cd6338773c2b99587b336a6141b185</td>\n",
       "      <td>gatas/gato&lt;n&gt;&lt;f&gt;&lt;pl&gt;</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12852941</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1363104812</td>\n",
       "      <td>285948</td>\n",
       "      <td>u:h3b5</td>\n",
       "      <td>pt</td>\n",
       "      <td>en</td>\n",
       "      <td>7ab5ef9586b79b2079fcce99fadd811f</td>\n",
       "      <td>cão/cão&lt;n&gt;&lt;m&gt;&lt;sg&gt;</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12852942</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1363104812</td>\n",
       "      <td>841</td>\n",
       "      <td>u:h3b5</td>\n",
       "      <td>pt</td>\n",
       "      <td>en</td>\n",
       "      <td>ba634cde4013c8ff4eee5892e92b3e5b</td>\n",
       "      <td>é/ser&lt;vbser&gt;&lt;pri&gt;&lt;p3&gt;&lt;sg&gt;</td>\n",
       "      <td>54</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12852943</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1363104812</td>\n",
       "      <td>841</td>\n",
       "      <td>u:h3b5</td>\n",
       "      <td>pt</td>\n",
       "      <td>en</td>\n",
       "      <td>630b709686520fc4f0d0e6077df220c3</td>\n",
       "      <td>bebe/beber&lt;vblex&gt;&lt;pri&gt;&lt;p3&gt;&lt;sg&gt;</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12852944</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1363104812</td>\n",
       "      <td>841</td>\n",
       "      <td>u:h3b5</td>\n",
       "      <td>pt</td>\n",
       "      <td>en</td>\n",
       "      <td>295086f6fa46c6554244540114f8aa71</td>\n",
       "      <td>escreve/escrever&lt;vblex&gt;&lt;pri&gt;&lt;p3&gt;&lt;sg&gt;</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311480 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          p_recall   timestamp   delta user_id learning_language ui_language  \\\n",
       "64             1.0  1362082493    1469  u:g3WM                pt          en   \n",
       "65             0.5  1362082493    1469  u:g3WM                pt          en   \n",
       "66             1.0  1362082493    1469  u:g3WM                pt          en   \n",
       "67             1.0  1362082493    2184  u:g3WM                pt          en   \n",
       "68             1.0  1362082493    1469  u:g3WM                pt          en   \n",
       "...            ...         ...     ...     ...               ...         ...   \n",
       "12852940       1.0  1363104812  604744  u:h3b5                pt          en   \n",
       "12852941       1.0  1363104812  285948  u:h3b5                pt          en   \n",
       "12852942       1.0  1363104812     841  u:h3b5                pt          en   \n",
       "12852943       1.0  1363104812     841  u:h3b5                pt          en   \n",
       "12852944       1.0  1363104812     841  u:h3b5                pt          en   \n",
       "\n",
       "                                 lexeme_id  \\\n",
       "64        57408f89412af98111a2f87c0ab41b22   \n",
       "65        8414835cb39e4315146a59fefdd6d1c6   \n",
       "66        ecc3feb8e53ce936cef181dd54e7aaca   \n",
       "67        8d28ba0fa188f1847571467189846dda   \n",
       "68        4b3613233b3fede2e3e92ac2ef752bf6   \n",
       "...                                    ...   \n",
       "12852940  09cd6338773c2b99587b336a6141b185   \n",
       "12852941  7ab5ef9586b79b2079fcce99fadd811f   \n",
       "12852942  ba634cde4013c8ff4eee5892e92b3e5b   \n",
       "12852943  630b709686520fc4f0d0e6077df220c3   \n",
       "12852944  295086f6fa46c6554244540114f8aa71   \n",
       "\n",
       "                                 lexeme_string  history_seen  history_correct  \\\n",
       "64                  tu/tu<prn><tn><p2><mf><sg>            48               48   \n",
       "65                 tem/ter<vblex><pri><p3><sg>             2                2   \n",
       "66               temos/ter<vblex><pri><p1><pl>             1                1   \n",
       "67                    tua/teu<det><pos><f><sg>             4                3   \n",
       "68                         leão/leão<n><m><sg>             9                8   \n",
       "...                                        ...           ...              ...   \n",
       "12852940                  gatas/gato<n><f><pl>            13               12   \n",
       "12852941                     cão/cão<n><m><sg>             8                7   \n",
       "12852942             é/ser<vbser><pri><p3><sg>            54               49   \n",
       "12852943        bebe/beber<vblex><pri><p3><sg>            27               27   \n",
       "12852944  escreve/escrever<vblex><pri><p3><sg>            25               25   \n",
       "\n",
       "          session_seen  session_correct  \n",
       "64                   1                1  \n",
       "65                   2                1  \n",
       "66                   1                1  \n",
       "67                   1                1  \n",
       "68                   1                1  \n",
       "...                ...              ...  \n",
       "12852940             1                1  \n",
       "12852941             1                1  \n",
       "12852942             1                1  \n",
       "12852943             2                2  \n",
       "12852944             1                1  \n",
       "\n",
       "[311480 rows x 12 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pt = df[df['learning_language'] == 'pt'].copy()\n",
    "df_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402bdb8c",
   "metadata": {},
   "source": [
    "## Calculate history accuracy and assaign lexeme code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0345b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt['history_acc_rate'] = df_pt['history_correct'] / (df_pt['history_seen'] + 0.0001)\n",
    "df_pt['lexeme_code'], uniques = pd.factorize(df_pt['lexeme_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3178e164",
   "metadata": {},
   "source": [
    "## Get the specific word for each lexeme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b2d90a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexeme_id</th>\n",
       "      <th>lexeme_code</th>\n",
       "      <th>word</th>\n",
       "      <th>global_correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57408f89412af98111a2f87c0ab41b22</td>\n",
       "      <td>0</td>\n",
       "      <td>tu/tu</td>\n",
       "      <td>0.969137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8414835cb39e4315146a59fefdd6d1c6</td>\n",
       "      <td>1</td>\n",
       "      <td>tem/ter</td>\n",
       "      <td>0.967542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ecc3feb8e53ce936cef181dd54e7aaca</td>\n",
       "      <td>2</td>\n",
       "      <td>temos/ter</td>\n",
       "      <td>0.982818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8d28ba0fa188f1847571467189846dda</td>\n",
       "      <td>3</td>\n",
       "      <td>tua/teu</td>\n",
       "      <td>0.920078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4b3613233b3fede2e3e92ac2ef752bf6</td>\n",
       "      <td>4</td>\n",
       "      <td>leão/leão</td>\n",
       "      <td>0.926518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          lexeme_id  lexeme_code       word  \\\n",
       "0  57408f89412af98111a2f87c0ab41b22            0      tu/tu   \n",
       "1  8414835cb39e4315146a59fefdd6d1c6            1    tem/ter   \n",
       "2  ecc3feb8e53ce936cef181dd54e7aaca            2  temos/ter   \n",
       "3  8d28ba0fa188f1847571467189846dda            3    tua/teu   \n",
       "4  4b3613233b3fede2e3e92ac2ef752bf6            4  leão/leão   \n",
       "\n",
       "   global_correctness  \n",
       "0            0.969137  \n",
       "1            0.967542  \n",
       "2            0.982818  \n",
       "3            0.920078  \n",
       "4            0.926518  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_word_from_lexeme_string(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Parse `word` from `lexeme_string`.\n",
    "\n",
    "    Rules you specified:\n",
    "    - Normal case (no '<*sf>' marker): take everything before the first '<'\n",
    "        'sua/seu<det><pos><f><sg>' -> 'sua/seu'\n",
    "    - If '<*sf>' occurs: take everything before the second '<'\n",
    "        '<*sf>/ter<vblex><pri><*pers><*numb>' -> '<*sf>/ter'\n",
    "      (i.e., include the '<*sf>' prefix and the lemma after '/')\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    s = str(s).strip()\n",
    "\n",
    "    if \"<*sf>\" not in s:\n",
    "        return s.split(\"<\", 1)[0].strip()\n",
    "\n",
    "    # With '<*sf>': return substring before the second '<'\n",
    "    first = s.find(\"<\")\n",
    "    if first == -1:\n",
    "        return s  # no tags at all\n",
    "    second = s.find(\"<\", first + 1)\n",
    "    if second == -1:\n",
    "        return s  # only one '<' found\n",
    "    return s[:second].strip()\n",
    "\n",
    "# 1) Create `word` parsed from lexeme_string\n",
    "df_word = df_pt.copy()\n",
    "df_word[\"word\"] = df_word[\"lexeme_string\"].apply(parse_word_from_lexeme_string)\n",
    "\n",
    "# 2) Aggregate per lexeme_id (and keep a consistent word)\n",
    "agg = (\n",
    "    df_word.groupby(\"lexeme_id\", as_index=False)\n",
    "      .agg(\n",
    "          lexeme_code=(\"lexeme_code\", \"first\"),\n",
    "          word=(\"word\", \"first\"),  # lexeme_id maps to one lexeme_string/word per your description\n",
    "          seen_sum=(\"history_seen\", \"sum\"),\n",
    "          correct_sum=(\"history_correct\", \"sum\"),\n",
    "      )\n",
    ")\n",
    "\n",
    "# 3) Compute global_correctness = sum(correct) / sum(seen)\n",
    "#    (avoid division by zero)\n",
    "agg[\"global_correctness\"] = agg[\"correct_sum\"] / (agg[\"seen_sum\"] + 0.0001)\n",
    "\n",
    "# 4) Final output table\n",
    "# 4) Final output + sort by lexeme_code ascending\n",
    "result = (\n",
    "    agg[[\"lexeme_id\", \"lexeme_code\", \"word\", \"global_correctness\"]]\n",
    "      .sort_values([\"lexeme_code\"], ascending=True)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdf5946",
   "metadata": {},
   "source": [
    "## Get the word frequecy in dataset B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "026d2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "TAG_RE = re.compile(r\"^<[^>]+>$\")  # like <*sf>\n",
    "\n",
    "def query_from_word(word: str) -> str | None:\n",
    "    if pd.isna(word):\n",
    "        return None\n",
    "    w = str(word).strip()\n",
    "    if not w:\n",
    "        return None\n",
    "\n",
    "    if \"/\" in w:\n",
    "        left, right = w.split(\"/\", 1)\n",
    "        left, right = left.strip(), right.strip()\n",
    "\n",
    "        # If left is a tag like <*sf>, use right\n",
    "        if TAG_RE.match(left):\n",
    "            q = right\n",
    "        else:\n",
    "            # In tem/ter, right is lemma; still use right by default\n",
    "            q = right\n",
    "    else:\n",
    "        q = w\n",
    "\n",
    "    # drop if query itself is a tag\n",
    "    if TAG_RE.match(q):\n",
    "        return None\n",
    "\n",
    "    return q.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85966d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasetB_txt(path: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    current_prompt = None\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "\n",
    "            # blank line: end of a prompt block\n",
    "            if not line:\n",
    "                current_prompt = None\n",
    "                continue\n",
    "\n",
    "            # prompt header line\n",
    "            if line.startswith(\"prompt_\") and \"|\" in line:\n",
    "                current_prompt = line.split(\"|\", 1)[0].strip()  # keep prompt_xxx id\n",
    "                continue\n",
    "\n",
    "            # translation line\n",
    "            if current_prompt is None:\n",
    "                # stray line; skip (or raise)\n",
    "                continue\n",
    "\n",
    "            if \"|\" not in line:\n",
    "                continue\n",
    "\n",
    "            trans, prob = line.rsplit(\"|\", 1)\n",
    "            try:\n",
    "                p = float(prob)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            rows.append({\"prompt_id\": current_prompt, \"translation\": trans, \"p\": p})\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ce5fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fairly safe tokenizer for Portuguese-ish text: letters with diacritics + apostrophe + hyphen\n",
    "TOKEN_RE = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ]+(?:[-'][A-Za-zÀ-ÖØ-öø-ÿ]+)*\", re.UNICODE)\n",
    "\n",
    "def tokens_from_translation(text: str) -> set[str]:\n",
    "    if pd.isna(text):\n",
    "        return set()\n",
    "    toks = TOKEN_RE.findall(str(text).lower())\n",
    "    return set(toks)\n",
    "\n",
    "def build_token_prompt_freq(dfB: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with columns: token, prompt_id, prompt_freq\n",
    "    where prompt_freq = sum(p of translations in that prompt containing token).\n",
    "    \"\"\"\n",
    "    tmp = dfB.copy()\n",
    "    tmp[\"token_set\"] = tmp[\"translation\"].apply(tokens_from_translation)\n",
    "\n",
    "    # explode token set -> one row per (prompt_id, token) per translation\n",
    "    exploded = tmp[[\"prompt_id\", \"p\", \"token_set\"]].explode(\"token_set\")\n",
    "    exploded = exploded.rename(columns={\"token_set\": \"token\"}).dropna(subset=[\"token\"])\n",
    "\n",
    "    # sum p within each prompt for each token\n",
    "    token_prompt = (\n",
    "        exploded.groupby([\"token\", \"prompt_id\"], as_index=False)[\"p\"]\n",
    "                .sum()\n",
    "                .rename(columns={\"p\": \"prompt_freq\"})\n",
    "    )\n",
    "    return token_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b81ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frequency_from_datasetB(result: pd.DataFrame, token_prompt: pd.DataFrame, total_prompts: int) -> pd.DataFrame:\n",
    "    out = result.copy()\n",
    "    out[\"query_token\"] = out[\"word\"].apply(query_from_word)\n",
    "\n",
    "    # For each token, compute:\n",
    "    # - prompt_count = number of prompts where it appears\n",
    "    # - frequency = mean(prompt_freq) across those prompts\n",
    "    stats = (\n",
    "        token_prompt.groupby(\"token\", as_index=False)\n",
    "                   .agg(\n",
    "                       prompt_count=(\"prompt_id\", \"nunique\"),\n",
    "                       frequency=(\"prompt_freq\", \"mean\"),\n",
    "                   )\n",
    "                   .rename(columns={\"token\": \"query_token\"})\n",
    "    )\n",
    "\n",
    "    out = out.merge(stats, on=\"query_token\", how=\"left\")\n",
    "    out[\"prompt_coverage\"] = out[\"prompt_count\"] / total_prompts\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4cf050ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge text files into one, dev + test + train\n",
    "files = [r\"C:\\Users\\lxa0530\\OneDrive\\Desktop\\Datathon\\Adding_B_feature\\input\\dev.en_pt.2020-02-20.gold.txt\", \n",
    "         r\"C:\\Users\\lxa0530\\OneDrive\\Desktop\\Datathon\\Adding_B_feature\\input\\test.en_pt.2020-02-20.gold.txt\", \n",
    "         r\"C:\\Users\\lxa0530\\OneDrive\\Desktop\\Datathon\\Adding_B_feature\\input\\train.en_pt.2020-01-13.gold.txt\"]\n",
    "out_file = r\"C:\\Users\\lxa0530\\OneDrive\\Desktop\\Datathon\\Adding_B_feature\\input\\merged_en_pt.txt\"\n",
    "\n",
    "with open(out_file, \"w\") as out:\n",
    "    for f in files:\n",
    "        with open(f, \"r\") as inp:\n",
    "            out.write(inp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4c65ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>prompt_001dbd157d83706b3cf32f34313ff3ab</td>\n",
       "      <td>0.197830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>prompt_0036b71bbdf9cb0046e200a7741924fb</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>prompt_005a9290311daddd1c9170c5916f9998</td>\n",
       "      <td>0.032080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>prompt_007cf192fbfca3c0f839fa90facbb28f</td>\n",
       "      <td>0.203200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>prompt_007e9fdefb3c7193d4a377b602620014</td>\n",
       "      <td>0.118014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token                                prompt_id  prompt_freq\n",
       "0     a  prompt_001dbd157d83706b3cf32f34313ff3ab     0.197830\n",
       "1     a  prompt_0036b71bbdf9cb0046e200a7741924fb     1.000000\n",
       "2     a  prompt_005a9290311daddd1c9170c5916f9998     0.032080\n",
       "3     a  prompt_007cf192fbfca3c0f839fa90facbb28f     0.203200\n",
       "4     a  prompt_007e9fdefb3c7193d4a377b602620014     0.118014"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_B = r\"C:\\Users\\lxa0530\\OneDrive\\Desktop\\Datathon\\merged_en_pt.txt\"\n",
    "dfB = load_datasetB_txt(path_B)\n",
    "total_prompts = dfB[\"prompt_id\"].nunique()\n",
    "token_prompt = build_token_prompt_freq(dfB)\n",
    "\n",
    "token_prompt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29084cba",
   "metadata": {},
   "source": [
    "## Adding frequency to the word df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b5dfe6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexeme_id</th>\n",
       "      <th>lexeme_code</th>\n",
       "      <th>word</th>\n",
       "      <th>global_correctness</th>\n",
       "      <th>query_token</th>\n",
       "      <th>prompt_count</th>\n",
       "      <th>frequency</th>\n",
       "      <th>prompt_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57408f89412af98111a2f87c0ab41b22</td>\n",
       "      <td>0</td>\n",
       "      <td>tu/tu</td>\n",
       "      <td>0.969137</td>\n",
       "      <td>tu</td>\n",
       "      <td>632.0</td>\n",
       "      <td>0.142486</td>\n",
       "      <td>0.1264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8414835cb39e4315146a59fefdd6d1c6</td>\n",
       "      <td>1</td>\n",
       "      <td>tem/ter</td>\n",
       "      <td>0.967542</td>\n",
       "      <td>ter</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.269861</td>\n",
       "      <td>0.0126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ecc3feb8e53ce936cef181dd54e7aaca</td>\n",
       "      <td>2</td>\n",
       "      <td>temos/ter</td>\n",
       "      <td>0.982818</td>\n",
       "      <td>ter</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.269861</td>\n",
       "      <td>0.0126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8d28ba0fa188f1847571467189846dda</td>\n",
       "      <td>3</td>\n",
       "      <td>tua/teu</td>\n",
       "      <td>0.920078</td>\n",
       "      <td>teu</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.195032</td>\n",
       "      <td>0.0212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4b3613233b3fede2e3e92ac2ef752bf6</td>\n",
       "      <td>4</td>\n",
       "      <td>leão/leão</td>\n",
       "      <td>0.926518</td>\n",
       "      <td>leão</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          lexeme_id  lexeme_code       word  \\\n",
       "0  57408f89412af98111a2f87c0ab41b22            0      tu/tu   \n",
       "1  8414835cb39e4315146a59fefdd6d1c6            1    tem/ter   \n",
       "2  ecc3feb8e53ce936cef181dd54e7aaca            2  temos/ter   \n",
       "3  8d28ba0fa188f1847571467189846dda            3    tua/teu   \n",
       "4  4b3613233b3fede2e3e92ac2ef752bf6            4  leão/leão   \n",
       "\n",
       "   global_correctness query_token  prompt_count  frequency  prompt_coverage  \n",
       "0            0.969137          tu         632.0   0.142486           0.1264  \n",
       "1            0.967542         ter          63.0   0.269861           0.0126  \n",
       "2            0.982818         ter          63.0   0.269861           0.0126  \n",
       "3            0.920078         teu         106.0   0.195032           0.0212  \n",
       "4            0.926518        leão           NaN        NaN              NaN  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_with_freq = add_frequency_from_datasetB(result, token_prompt, total_prompts)\n",
    "result_with_freq = result_with_freq.sort_values(\"lexeme_code\").reset_index(drop=True)\n",
    "\n",
    "result_with_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "55da43d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(377)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_with_freq['prompt_count'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceda998",
   "metadata": {},
   "source": [
    "There are 377 words out of 2815 never appear in dataset B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ecb838",
   "metadata": {},
   "source": [
    "## Calculate weight for each word\n",
    "$$\n",
    "\\mathrm{weight\\_raw}\n",
    "=\n",
    "\\left(4p(1-p)\\right)^{\\beta}\n",
    "\\cdot\n",
    "\\left(\\sqrt{f}\\right)^{\\delta}\n",
    "\\cdot\n",
    "\\left(\\sqrt{\\mathrm{cov}}\\right)^{\\gamma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b03c9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_weight_table(\n",
    "    word_with_freq: pd.DataFrame,\n",
    "    beta: float = 1.0,\n",
    "    delta: float = 0.5,\n",
    "    gamma: float = 0.5,\n",
    "    freq_default: float = 0.01,  # <- NaN frequency uses this, not 0\n",
    "    cov_default: float = 0.01,   # <- NaN coverage uses this, not 0\n",
    "    normalize: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    df = word_with_freq.copy()\n",
    "\n",
    "    # correctness\n",
    "    p = df[\"global_correctness\"].astype(float).clip(0, 1).fillna(0.0)\n",
    "    w_disc = 4.0 * p * (1.0 - p)\n",
    "\n",
    "    # frequency: NaN -> default; otherwise keep (and clip)\n",
    "    f = df[\"frequency\"].astype(float)\n",
    "    f = f.where(~f.isna(), freq_default).clip(lower=0)\n",
    "\n",
    "    # coverage: NaN -> default\n",
    "    cov = df[\"prompt_coverage\"].astype(float)\n",
    "    cov = cov.where(~cov.isna(), cov_default).clip(0, 1)\n",
    "\n",
    "    w_freq = np.sqrt(f)\n",
    "    w_cov  = np.sqrt(cov)\n",
    "\n",
    "    df[\"weight_raw\"] = (w_disc ** beta) * (w_freq ** delta) * (w_cov ** gamma)\n",
    "\n",
    "    if normalize:\n",
    "        mx = df[\"weight_raw\"].max()\n",
    "        df[\"weight\"] = df[\"weight_raw\"] / mx if mx and np.isfinite(mx) else 0.0\n",
    "    else:\n",
    "        df[\"weight\"] = df[\"weight_raw\"]\n",
    "\n",
    "    cols = [c for c in [\"lexeme_id\",\"lexeme_code\",\"word\",\n",
    "                        \"global_correctness\",\"frequency\",\"prompt_coverage\",\n",
    "                        \"weight\"] if c in df.columns]\n",
    "    return df[cols].sort_values([\"lexeme_code\",\"lexeme_id\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "word_weight_table = build_word_weight_table(result_with_freq, freq_default=0.01, cov_default=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d488c7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexeme_id</th>\n",
       "      <th>lexeme_code</th>\n",
       "      <th>word</th>\n",
       "      <th>global_correctness</th>\n",
       "      <th>frequency</th>\n",
       "      <th>prompt_coverage</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57408f89412af98111a2f87c0ab41b22</td>\n",
       "      <td>0</td>\n",
       "      <td>tu/tu</td>\n",
       "      <td>0.969137</td>\n",
       "      <td>0.142486</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>0.067243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8414835cb39e4315146a59fefdd6d1c6</td>\n",
       "      <td>1</td>\n",
       "      <td>tem/ter</td>\n",
       "      <td>0.967542</td>\n",
       "      <td>0.269861</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.046538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ecc3feb8e53ce936cef181dd54e7aaca</td>\n",
       "      <td>2</td>\n",
       "      <td>temos/ter</td>\n",
       "      <td>0.982818</td>\n",
       "      <td>0.269861</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.025025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8d28ba0fa188f1847571467189846dda</td>\n",
       "      <td>3</td>\n",
       "      <td>tua/teu</td>\n",
       "      <td>0.920078</td>\n",
       "      <td>0.195032</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.114432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4b3613233b3fede2e3e92ac2ef752bf6</td>\n",
       "      <td>4</td>\n",
       "      <td>leão/leão</td>\n",
       "      <td>0.926518</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          lexeme_id  lexeme_code       word  \\\n",
       "0  57408f89412af98111a2f87c0ab41b22            0      tu/tu   \n",
       "1  8414835cb39e4315146a59fefdd6d1c6            1    tem/ter   \n",
       "2  ecc3feb8e53ce936cef181dd54e7aaca            2  temos/ter   \n",
       "3  8d28ba0fa188f1847571467189846dda            3    tua/teu   \n",
       "4  4b3613233b3fede2e3e92ac2ef752bf6            4  leão/leão   \n",
       "\n",
       "   global_correctness  frequency  prompt_coverage    weight  \n",
       "0            0.969137   0.142486           0.1264  0.067243  \n",
       "1            0.967542   0.269861           0.0126  0.046538  \n",
       "2            0.982818   0.269861           0.0126  0.025025  \n",
       "3            0.920078   0.195032           0.0212  0.114432  \n",
       "4            0.926518        NaN              NaN  0.041781  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_weight_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9bc38d",
   "metadata": {},
   "source": [
    "## Use user word correctness and word weight to calculate user feature scores\n",
    "$$\n",
    "\\mathrm{feature\\_score}(u,l)\n",
    "=\n",
    "r(u,l)\\cdot \\sqrt{w(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7649b43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>lexeme_code</th>\n",
       "      <th>feature_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u:g3WM</td>\n",
       "      <td>0</td>\n",
       "      <td>0.259312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u:g3WM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.215716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u:g3WM</td>\n",
       "      <td>2</td>\n",
       "      <td>0.158178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u:g3WM</td>\n",
       "      <td>3</td>\n",
       "      <td>0.253702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u:g3WM</td>\n",
       "      <td>4</td>\n",
       "      <td>0.181691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id  lexeme_code  feature_score\n",
       "0  u:g3WM            0       0.259312\n",
       "1  u:g3WM            1       0.215716\n",
       "2  u:g3WM            2       0.158178\n",
       "3  u:g3WM            3       0.253702\n",
       "4  u:g3WM            4       0.181691"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_user_lexeme_features_A(\n",
    "    user_lexeme_df: pd.DataFrame,\n",
    "    lexeme_weight_df: pd.DataFrame,\n",
    "    recall_col: str = \"history_acc_rate\",\n",
    "    normalize_weight: bool = False,   # 如果你的 weight 已经是 0-1，就不用再 normalize\n",
    ") -> pd.DataFrame:\n",
    "    u = user_lexeme_df.copy()\n",
    "    w = lexeme_weight_df.copy()\n",
    "\n",
    "    # optional: normalize weight to [0,1] within this table\n",
    "    if normalize_weight:\n",
    "        mx = w[\"weight\"].max()\n",
    "        w[\"weight\"] = w[\"weight\"] / mx if mx and np.isfinite(mx) else 0.0\n",
    "\n",
    "    # keep needed columns, dedupe lexeme_code in weight table\n",
    "    w = w[[\"lexeme_code\", \"weight\", \"global_correctness\"]].drop_duplicates(\"lexeme_code\")\n",
    "\n",
    "    # left join: only user-lexeme pairs that exist in user table are produced (sparse)\n",
    "    x = u.merge(w, on=\"lexeme_code\", how=\"left\")\n",
    "\n",
    "    # handle missing weight/global_correctness (if any lexemes not found in w)\n",
    "    # you can decide to drop them or set safe defaults:\n",
    "    x[\"weight\"] = x[\"weight\"].astype(float).fillna(0.0).clip(lower=0.0)\n",
    "    x[\"global_correctness\"] = x[\"global_correctness\"].astype(float).fillna(0.0).clip(0.0, 1.0)\n",
    "\n",
    "    # clean recall\n",
    "    x[recall_col] = x[recall_col].astype(float).fillna(0.0).clip(0.0, 1.0)\n",
    "\n",
    "    # feature score\n",
    "    #x[\"feature_score\"] = (x[recall_col] - x[\"global_correctness\"]) * np.sqrt(x[\"weight\"])\n",
    "    x[\"feature_score\"] = x[recall_col] * np.sqrt(x[\"weight\"])\n",
    "    # output sparse feature table\n",
    "    out = x[[\"user_id\", \"lexeme_code\", \"feature_score\"]].copy()\n",
    "    return out\n",
    "\n",
    "# usage\n",
    "user_lexeme_feature_sparse = build_user_lexeme_features_A(\n",
    "    user_lexeme_df=df_pt,\n",
    "    lexeme_weight_df=word_weight_table,  # must include global_correctness\n",
    ")\n",
    "user_lexeme_feature_sparse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0261f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature_matrix = (\n",
    "    user_lexeme_feature_sparse\n",
    "      .pivot_table(index=\"user_id\", columns=\"lexeme_code\", values=\"feature_score\", fill_value=0.0)\n",
    ")\n",
    "lexeme_codes = user_feature_matrix.columns.tolist()\n",
    "col_map = {code: f\"lexeme_{i}\" for i, code in enumerate(lexeme_codes)}\n",
    "\n",
    "user_feature_matrix_renamed = user_feature_matrix.rename(columns=col_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aacb5432",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature_matrix_renamed.to_csv(r'C:\\Users\\lxa0530\\OneDrive\\Desktop\\Datathon\\Adding_B_feature\\output\\user_feature_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab0e583b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>lexeme_code</th>\n",
       "      <th>lexeme_0</th>\n",
       "      <th>lexeme_1</th>\n",
       "      <th>lexeme_2</th>\n",
       "      <th>lexeme_3</th>\n",
       "      <th>lexeme_4</th>\n",
       "      <th>lexeme_5</th>\n",
       "      <th>lexeme_6</th>\n",
       "      <th>lexeme_7</th>\n",
       "      <th>lexeme_8</th>\n",
       "      <th>lexeme_9</th>\n",
       "      <th>...</th>\n",
       "      <th>lexeme_2805</th>\n",
       "      <th>lexeme_2806</th>\n",
       "      <th>lexeme_2807</th>\n",
       "      <th>lexeme_2808</th>\n",
       "      <th>lexeme_2809</th>\n",
       "      <th>lexeme_2810</th>\n",
       "      <th>lexeme_2811</th>\n",
       "      <th>lexeme_2812</th>\n",
       "      <th>lexeme_2813</th>\n",
       "      <th>lexeme_2814</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>u:0X2</th>\n",
       "      <td>0.259311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u:0b</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u:0xw</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.487101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u:1EH</th>\n",
       "      <td>0.259311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.204402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.365346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u:1gx</th>\n",
       "      <td>0.259311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "lexeme_code  lexeme_0  lexeme_1  lexeme_2  lexeme_3  lexeme_4  lexeme_5  \\\n",
       "user_id                                                                   \n",
       "u:0X2        0.259311       0.0       0.0       0.0  0.000000       0.0   \n",
       "u:0b         0.000000       0.0       0.0       0.0  0.000000       0.0   \n",
       "u:0xw        0.000000       0.0       0.0       0.0  0.000000       0.0   \n",
       "u:1EH        0.259311       0.0       0.0       0.0  0.204402       0.0   \n",
       "u:1gx        0.259311       0.0       0.0       0.0  0.000000       0.0   \n",
       "\n",
       "lexeme_code  lexeme_6  lexeme_7  lexeme_8  lexeme_9  ...  lexeme_2805  \\\n",
       "user_id                                              ...                \n",
       "u:0X2        0.000000       0.0       0.0       0.0  ...          0.0   \n",
       "u:0b         0.000000       0.0       0.0       0.0  ...          0.0   \n",
       "u:0xw        0.487101       0.0       0.0       0.0  ...          0.0   \n",
       "u:1EH        0.365346       0.0       0.0       0.0  ...          0.0   \n",
       "u:1gx        0.324745       0.0       0.0       0.0  ...          0.0   \n",
       "\n",
       "lexeme_code  lexeme_2806  lexeme_2807  lexeme_2808  lexeme_2809  lexeme_2810  \\\n",
       "user_id                                                                        \n",
       "u:0X2                0.0          0.0          0.0          0.0          0.0   \n",
       "u:0b                 0.0          0.0          0.0          0.0          0.0   \n",
       "u:0xw                0.0          0.0          0.0          0.0          0.0   \n",
       "u:1EH                0.0          0.0          0.0          0.0          0.0   \n",
       "u:1gx                0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "lexeme_code  lexeme_2811  lexeme_2812  lexeme_2813  lexeme_2814  \n",
       "user_id                                                          \n",
       "u:0X2                0.0          0.0          0.0          0.0  \n",
       "u:0b                 0.0          0.0          0.0          0.0  \n",
       "u:0xw                0.0          0.0          0.0          0.0  \n",
       "u:1EH                0.0          0.0          0.0          0.0  \n",
       "u:1gx                0.0          0.0          0.0          0.0  \n",
       "\n",
       "[5 rows x 2815 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_feature_matrix_renamed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754bb149",
   "metadata": {},
   "source": [
    "## Adding other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "34e6f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path = r\"C:\\Users\\lxa0530\\OneDrive\\Desktop\\Datathon\\Adding_B_feature\\users_fingerprint.csv\"\n",
    "user_feature = pd.read_csv(feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e1396e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path_B = r\"C:\\Users\\lxa0530\\OneDrive\\Desktop\\Datathon\\Adding_B_feature\\output\\user_feature_matrix.csv\"\n",
    "user_feature_B = pd.read_csv(feature_path_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e9b8f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>lexeme_0</th>\n",
       "      <th>lexeme_1</th>\n",
       "      <th>lexeme_2</th>\n",
       "      <th>lexeme_3</th>\n",
       "      <th>lexeme_4</th>\n",
       "      <th>lexeme_5</th>\n",
       "      <th>lexeme_6</th>\n",
       "      <th>lexeme_7</th>\n",
       "      <th>lexeme_8</th>\n",
       "      <th>...</th>\n",
       "      <th>lexeme_2805</th>\n",
       "      <th>lexeme_2806</th>\n",
       "      <th>lexeme_2807</th>\n",
       "      <th>lexeme_2808</th>\n",
       "      <th>lexeme_2809</th>\n",
       "      <th>lexeme_2810</th>\n",
       "      <th>lexeme_2811</th>\n",
       "      <th>lexeme_2812</th>\n",
       "      <th>lexeme_2813</th>\n",
       "      <th>lexeme_2814</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u:0X2</td>\n",
       "      <td>0.259311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u:0b</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u:0xw</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.487101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u:1EH</td>\n",
       "      <td>0.259311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.204402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.365346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u:1gx</td>\n",
       "      <td>0.259311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>u:yT9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>u:yyO</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>u:z4x</td>\n",
       "      <td>0.248016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.175201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.487085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>u:zmi</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>u:zzU</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.487122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2709 rows × 2816 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  lexeme_0  lexeme_1  lexeme_2  lexeme_3  lexeme_4  lexeme_5  \\\n",
       "0      u:0X2  0.259311       0.0       0.0       0.0  0.000000       0.0   \n",
       "1       u:0b  0.000000       0.0       0.0       0.0  0.000000       0.0   \n",
       "2      u:0xw  0.000000       0.0       0.0       0.0  0.000000       0.0   \n",
       "3      u:1EH  0.259311       0.0       0.0       0.0  0.204402       0.0   \n",
       "4      u:1gx  0.259311       0.0       0.0       0.0  0.000000       0.0   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "2704   u:yT9  0.000000       0.0       0.0       0.0  0.000000       0.0   \n",
       "2705   u:yyO  0.000000       0.0       0.0       0.0  0.000000       0.0   \n",
       "2706   u:z4x  0.248016       0.0       0.0       0.0  0.175201       0.0   \n",
       "2707   u:zmi  0.000000       0.0       0.0       0.0  0.000000       0.0   \n",
       "2708   u:zzU  0.000000       0.0       0.0       0.0  0.000000       0.0   \n",
       "\n",
       "      lexeme_6  lexeme_7  lexeme_8  ...  lexeme_2805  lexeme_2806  \\\n",
       "0     0.000000       0.0       0.0  ...          0.0          0.0   \n",
       "1     0.000000       0.0       0.0  ...          0.0          0.0   \n",
       "2     0.487101       0.0       0.0  ...          0.0          0.0   \n",
       "3     0.365346       0.0       0.0  ...          0.0          0.0   \n",
       "4     0.324745       0.0       0.0  ...          0.0          0.0   \n",
       "...        ...       ...       ...  ...          ...          ...   \n",
       "2704  0.000000       0.0       0.0  ...          0.0          0.0   \n",
       "2705  0.000000       0.0       0.0  ...          0.0          0.0   \n",
       "2706  0.487085       0.0       0.0  ...          0.0          0.0   \n",
       "2707  0.324745       0.0       0.0  ...          0.0          0.0   \n",
       "2708  0.487122       0.0       0.0  ...          0.0          0.0   \n",
       "\n",
       "      lexeme_2807  lexeme_2808  lexeme_2809  lexeme_2810  lexeme_2811  \\\n",
       "0             0.0          0.0          0.0          0.0          0.0   \n",
       "1             0.0          0.0          0.0          0.0          0.0   \n",
       "2             0.0          0.0          0.0          0.0          0.0   \n",
       "3             0.0          0.0          0.0          0.0          0.0   \n",
       "4             0.0          0.0          0.0          0.0          0.0   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "2704          0.0          0.0          0.0          0.0          0.0   \n",
       "2705          0.0          0.0          0.0          0.0          0.0   \n",
       "2706          0.0          0.0          0.0          0.0          0.0   \n",
       "2707          0.0          0.0          0.0          0.0          0.0   \n",
       "2708          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "      lexeme_2812  lexeme_2813  lexeme_2814  \n",
       "0             0.0          0.0          0.0  \n",
       "1             0.0          0.0          0.0  \n",
       "2             0.0          0.0          0.0  \n",
       "3             0.0          0.0          0.0  \n",
       "4             0.0          0.0          0.0  \n",
       "...           ...          ...          ...  \n",
       "2704          0.0          0.0          0.0  \n",
       "2705          0.0          0.0          0.0  \n",
       "2706          0.0          0.0          0.0  \n",
       "2707          0.0          0.0          0.0  \n",
       "2708          0.0          0.0          0.0  \n",
       "\n",
       "[2709 rows x 2816 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_feature_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a9090a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>max_history_seen</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>learning_speed</th>\n",
       "      <th>lexeme_0</th>\n",
       "      <th>lexeme_1</th>\n",
       "      <th>lexeme_2</th>\n",
       "      <th>lexeme_3</th>\n",
       "      <th>lexeme_4</th>\n",
       "      <th>...</th>\n",
       "      <th>lexeme_2805_seen</th>\n",
       "      <th>lexeme_2806_seen</th>\n",
       "      <th>lexeme_2807_seen</th>\n",
       "      <th>lexeme_2808_seen</th>\n",
       "      <th>lexeme_2809_seen</th>\n",
       "      <th>lexeme_2810_seen</th>\n",
       "      <th>lexeme_2811_seen</th>\n",
       "      <th>lexeme_2812_seen</th>\n",
       "      <th>lexeme_2813_seen</th>\n",
       "      <th>lexeme_2814_seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>u:0X2</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>13.563969</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>u:0b</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3.629236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>u:0xw</td>\n",
       "      <td>20</td>\n",
       "      <td>108</td>\n",
       "      <td>2.633296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>u:1EH</td>\n",
       "      <td>14</td>\n",
       "      <td>52</td>\n",
       "      <td>2.580331</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>u:1gx</td>\n",
       "      <td>15</td>\n",
       "      <td>71</td>\n",
       "      <td>7.584663</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>2704</td>\n",
       "      <td>u:yT9</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>13.931693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>2705</td>\n",
       "      <td>u:yyO</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>2.044361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>2706</td>\n",
       "      <td>u:z4x</td>\n",
       "      <td>85</td>\n",
       "      <td>83</td>\n",
       "      <td>2.375734</td>\n",
       "      <td>0.958847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.868058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>2707</td>\n",
       "      <td>u:zmi</td>\n",
       "      <td>12</td>\n",
       "      <td>42</td>\n",
       "      <td>8.380809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>2708</td>\n",
       "      <td>u:zzU</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>13.669329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2709 rows × 5635 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 user_id  max_history_seen  vocab_size  learning_speed  \\\n",
       "0              0   u:0X2                15           9       13.563969   \n",
       "1              1    u:0b                12          12        3.629236   \n",
       "2              2   u:0xw                20         108        2.633296   \n",
       "3              3   u:1EH                14          52        2.580331   \n",
       "4              4   u:1gx                15          71        7.584663   \n",
       "...          ...     ...               ...         ...             ...   \n",
       "2704        2704   u:yT9                20          13       13.931693   \n",
       "2705        2705   u:yyO                21          14        2.044361   \n",
       "2706        2706   u:z4x                85          83        2.375734   \n",
       "2707        2707   u:zmi                12          42        8.380809   \n",
       "2708        2708   u:zzU                 6          10       13.669329   \n",
       "\n",
       "      lexeme_0  lexeme_1  lexeme_2  lexeme_3  lexeme_4  ...  lexeme_2805_seen  \\\n",
       "0     0.999994       0.0       0.0       0.0  0.000000  ...               0.0   \n",
       "1     0.000000       0.0       0.0       0.0  0.000000  ...               0.0   \n",
       "2     0.000000       0.0       0.0       0.0  0.000000  ...               0.0   \n",
       "3     0.999992       0.0       0.0       0.0  0.999987  ...               0.0   \n",
       "4     0.999994       0.0       0.0       0.0  0.000000  ...               0.0   \n",
       "...        ...       ...       ...       ...       ...  ...               ...   \n",
       "2704  0.000000       0.0       0.0       0.0  0.000000  ...               0.0   \n",
       "2705  0.000000       0.0       0.0       0.0  0.000000  ...               0.0   \n",
       "2706  0.958847       0.0       0.0       0.0  0.868058  ...               0.0   \n",
       "2707  0.000000       0.0       0.0       0.0  0.000000  ...               0.0   \n",
       "2708  0.000000       0.0       0.0       0.0  0.000000  ...               0.0   \n",
       "\n",
       "      lexeme_2806_seen  lexeme_2807_seen  lexeme_2808_seen  lexeme_2809_seen  \\\n",
       "0                  0.0               0.0               0.0               0.0   \n",
       "1                  0.0               0.0               0.0               0.0   \n",
       "2                  0.0               0.0               0.0               0.0   \n",
       "3                  0.0               0.0               0.0               0.0   \n",
       "4                  0.0               0.0               0.0               0.0   \n",
       "...                ...               ...               ...               ...   \n",
       "2704               0.0               0.0               0.0               0.0   \n",
       "2705               0.0               0.0               0.0               0.0   \n",
       "2706               0.0               0.0               0.0               0.0   \n",
       "2707               0.0               0.0               0.0               0.0   \n",
       "2708               0.0               0.0               0.0               0.0   \n",
       "\n",
       "      lexeme_2810_seen  lexeme_2811_seen  lexeme_2812_seen  lexeme_2813_seen  \\\n",
       "0                  0.0               0.0               0.0               0.0   \n",
       "1                  0.0               0.0               0.0               0.0   \n",
       "2                  0.0               0.0               0.0               0.0   \n",
       "3                  0.0               0.0               0.0               0.0   \n",
       "4                  0.0               0.0               0.0               0.0   \n",
       "...                ...               ...               ...               ...   \n",
       "2704               0.0               0.0               0.0               0.0   \n",
       "2705               0.0               0.0               0.0               0.0   \n",
       "2706               0.0               0.0               0.0               0.0   \n",
       "2707               0.0               0.0               0.0               0.0   \n",
       "2708               0.0               0.0               0.0               0.0   \n",
       "\n",
       "      lexeme_2814_seen  \n",
       "0                  0.0  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "3                  0.0  \n",
       "4                  0.0  \n",
       "...                ...  \n",
       "2704               0.0  \n",
       "2705               0.0  \n",
       "2706               0.0  \n",
       "2707               0.0  \n",
       "2708               0.0  \n",
       "\n",
       "[2709 rows x 5635 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a301c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_cols = [f\"lexeme_{i}_seen\" for i in range(2815)]  # lexeme_0_seen ... lexeme_2814_seen\n",
    "lexeme_cols = [f\"lexeme_{i}\" for i in range(2815)]    # lexeme_0 ... lexeme_2814\n",
    "\n",
    "user_feature_keep = (\n",
    "    [\"user_id\", \"max_history_seen\", \"vocab_size\", \"learning_speed\"]\n",
    "    + seen_cols\n",
    ")\n",
    "user_feature_B_keep = [\"user_id\"] + lexeme_cols\n",
    "\n",
    "df1 = user_feature.loc[:, user_feature_keep]\n",
    "df2 = user_feature_B.loc[:, user_feature_B_keep]\n",
    "\n",
    "merged = df1.merge(df2, on=\"user_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9a2a8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(r\"C:\\Users\\lxa0530\\OneDrive\\Desktop\\Datathon\\Adding_B_feature\\output\\user_fingerprint_B.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8a37f4",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4df75db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select the columns to be formalized. \n",
    "cols_to_scale = merged.columns.drop(\"user_id\")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = merged.copy()\n",
    "df_scaled[cols_to_scale] = scaler.fit_transform(merged[cols_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d6a0d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.to_csv(r\"C:\\Users\\lxa0530\\OneDrive\\Desktop\\Datathon\\Adding_B_feature\\output\\user_fingerprint_B_scaled.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
