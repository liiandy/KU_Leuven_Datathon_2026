{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e38b69ac",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction of Knowledge & Exposure Fingerprint Features\n",
    "\n",
    "- Main aim is to reduce dimensionality of these feature by grouping semantically similar lexemes into clusters that we can aggregate scores into\n",
    "- Use [CALE-XLLEX](https://huggingface.co/gabrielloiseau/CALE-XLLEX) from Huggingface to generate emeddings\n",
    "- Add in morphological tags as onehotencoded features\n",
    "- Cluster using HDBSCAN / k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b5fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a50e6",
   "metadata": {},
   "source": [
    "#### Load lexeme_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d33f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 12854226 entries, 0 to 12854225\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   p_recall           float64\n",
      " 1   timestamp          int64  \n",
      " 2   delta              int64  \n",
      " 3   user_id            str    \n",
      " 4   learning_language  str    \n",
      " 5   ui_language        str    \n",
      " 6   lexeme_id          str    \n",
      " 7   lexeme_string      str    \n",
      " 8   history_seen       int64  \n",
      " 9   history_correct    int64  \n",
      " 10  session_seen       int64  \n",
      " 11  session_correct    int64  \n",
      "dtypes: float64(1), int64(6), str(5)\n",
      "memory usage: 1.9 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_a_path = \"/Users/andrea/Documents/duolingo_hack/dataset_a_spacedrepetition.csv\"\n",
    "\n",
    "dataset_a_raw = pd.read_csv(dataset_a_path)\n",
    "dataset_a_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b7be3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2815/2815 [00:00<00:00, 626497.17it/s]\n",
      "100%|██████████| 2815/2815 [00:00<00:00, 466660.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lexeme_id        2815\n",
       "lexeme_string    2815\n",
       "word             2256\n",
       "word_sf          1600\n",
       "word_clean       1992\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.preprocess_lexeme import parse_morph_tags_from_lexeme_string, parse_word_from_lexeme_string\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "lexeme_df = dataset_a_raw.loc[dataset_a_raw.learning_language==\"pt\",['lexeme_id', 'lexeme_string']].drop_duplicates().assign(\n",
    "    word = lambda x: x.lexeme_string.progress_apply(parse_word_from_lexeme_string),\n",
    "    word_sf = lambda x: x.word.str.replace(\"^.*/\", \"\", regex=True),\n",
    "    word_clean = lambda x: x.word.str.replace(\"/.*$\", \"\", regex=True),\n",
    "    morph_tags = lambda x: x.lexeme_string.progress_apply(parse_morph_tags_from_lexeme_string)\n",
    ")\n",
    "\n",
    "lexeme_df.loc[lexeme_df.word_clean==\"<*sf>\", \"word_clean\"] = lexeme_df.loc[lexeme_df.word_clean==\"<*sf>\", \"word_sf\"]\n",
    "lexeme_df.drop(columns=\"morph_tags\").nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ffc05c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexeme_id</th>\n",
       "      <th>lexeme_string</th>\n",
       "      <th>word</th>\n",
       "      <th>word_sf</th>\n",
       "      <th>word_clean</th>\n",
       "      <th>morph_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79107</th>\n",
       "      <td>87a2035d3e8db90a09b6f0e918e27422</td>\n",
       "      <td>&lt;*sf&gt;/abrir&lt;vblex&gt;&lt;pri&gt;&lt;*pers&gt;&lt;*numb&gt;</td>\n",
       "      <td>&lt;*sf&gt;/abrir</td>\n",
       "      <td>abrir</td>\n",
       "      <td>abrir</td>\n",
       "      <td>[&lt;*sf&gt;, &lt;vblex&gt;, &lt;pri&gt;, &lt;*pers&gt;, &lt;*numb&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968967</th>\n",
       "      <td>78a3dc9a89d3269c9cb3908185b72b76</td>\n",
       "      <td>&lt;*sf&gt;/abrir&lt;vblex&gt;&lt;pp&gt;&lt;*gndr&gt;&lt;*numb&gt;&lt;@present_...</td>\n",
       "      <td>&lt;*sf&gt;/abrir</td>\n",
       "      <td>abrir</td>\n",
       "      <td>abrir</td>\n",
       "      <td>[&lt;*sf&gt;, &lt;vblex&gt;, &lt;pp&gt;, &lt;*gndr&gt;, &lt;*numb&gt;, &lt;@pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4097724</th>\n",
       "      <td>aca35a89d2157d6aa3f483d2349d2694</td>\n",
       "      <td>&lt;*sf&gt;/abrir&lt;vblex&gt;&lt;cni&gt;&lt;*pers&gt;&lt;*numb&gt;</td>\n",
       "      <td>&lt;*sf&gt;/abrir</td>\n",
       "      <td>abrir</td>\n",
       "      <td>abrir</td>\n",
       "      <td>[&lt;*sf&gt;, &lt;vblex&gt;, &lt;cni&gt;, &lt;*pers&gt;, &lt;*numb&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5187118</th>\n",
       "      <td>9411c14227976e814c749377bbdd99df</td>\n",
       "      <td>&lt;*sf&gt;/abrir&lt;vblex&gt;&lt;ifi&gt;&lt;*pers&gt;&lt;*numb&gt;</td>\n",
       "      <td>&lt;*sf&gt;/abrir</td>\n",
       "      <td>abrir</td>\n",
       "      <td>abrir</td>\n",
       "      <td>[&lt;*sf&gt;, &lt;vblex&gt;, &lt;ifi&gt;, &lt;*pers&gt;, &lt;*numb&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11676766</th>\n",
       "      <td>618939eca4e920e91a53c9069a9a7c9c</td>\n",
       "      <td>&lt;*sf&gt;/abrir&lt;vblex&gt;&lt;fti&gt;&lt;*pers&gt;&lt;*numb&gt;</td>\n",
       "      <td>&lt;*sf&gt;/abrir</td>\n",
       "      <td>abrir</td>\n",
       "      <td>abrir</td>\n",
       "      <td>[&lt;*sf&gt;, &lt;vblex&gt;, &lt;fti&gt;, &lt;*pers&gt;, &lt;*numb&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11080420</th>\n",
       "      <td>c4521713a55acd03761a16885a4d284f</td>\n",
       "      <td>vai/ir&lt;vblex&gt;&lt;pri&gt;&lt;p3&gt;&lt;sg&gt;&lt;@future_phrasal&gt;</td>\n",
       "      <td>vai/ir</td>\n",
       "      <td>ir</td>\n",
       "      <td>vai</td>\n",
       "      <td>[&lt;vblex&gt;, &lt;pri&gt;, &lt;p3&gt;, &lt;sg&gt;, &lt;@future_phrasal&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113367</th>\n",
       "      <td>1acf23ffbb788d259351dc5e9cb749f8</td>\n",
       "      <td>vocês/você&lt;prn&gt;&lt;tn&gt;&lt;p3&gt;&lt;mf&gt;&lt;pl&gt;</td>\n",
       "      <td>vocês/você</td>\n",
       "      <td>você</td>\n",
       "      <td>vocês</td>\n",
       "      <td>[&lt;prn&gt;, &lt;tn&gt;, &lt;p3&gt;, &lt;mf&gt;, &lt;pl&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9052892</th>\n",
       "      <td>39bb5ac0043764254a436992e7ed181b</td>\n",
       "      <td>vocês/você&lt;prn&gt;&lt;tn&gt;&lt;p2&gt;&lt;mf&gt;&lt;pl&gt;</td>\n",
       "      <td>vocês/você</td>\n",
       "      <td>você</td>\n",
       "      <td>vocês</td>\n",
       "      <td>[&lt;prn&gt;, &lt;tn&gt;, &lt;p2&gt;, &lt;mf&gt;, &lt;pl&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913339</th>\n",
       "      <td>835d261ae0439c3956f028e35230e190</td>\n",
       "      <td>voltar/voltar&lt;vblex&gt;&lt;inf&gt;&lt;@future_phrasal&gt;</td>\n",
       "      <td>voltar/voltar</td>\n",
       "      <td>voltar</td>\n",
       "      <td>voltar</td>\n",
       "      <td>[&lt;vblex&gt;, &lt;inf&gt;, &lt;@future_phrasal&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5669369</th>\n",
       "      <td>b1067bd96921050f31e4309d269d87cd</td>\n",
       "      <td>voltar/voltar&lt;vblex&gt;&lt;inf&gt;</td>\n",
       "      <td>voltar/voltar</td>\n",
       "      <td>voltar</td>\n",
       "      <td>voltar</td>\n",
       "      <td>[&lt;vblex&gt;, &lt;inf&gt;]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>755 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 lexeme_id  \\\n",
       "79107     87a2035d3e8db90a09b6f0e918e27422   \n",
       "968967    78a3dc9a89d3269c9cb3908185b72b76   \n",
       "4097724   aca35a89d2157d6aa3f483d2349d2694   \n",
       "5187118   9411c14227976e814c749377bbdd99df   \n",
       "11676766  618939eca4e920e91a53c9069a9a7c9c   \n",
       "...                                    ...   \n",
       "11080420  c4521713a55acd03761a16885a4d284f   \n",
       "113367    1acf23ffbb788d259351dc5e9cb749f8   \n",
       "9052892   39bb5ac0043764254a436992e7ed181b   \n",
       "913339    835d261ae0439c3956f028e35230e190   \n",
       "5669369   b1067bd96921050f31e4309d269d87cd   \n",
       "\n",
       "                                              lexeme_string           word  \\\n",
       "79107                 <*sf>/abrir<vblex><pri><*pers><*numb>    <*sf>/abrir   \n",
       "968967    <*sf>/abrir<vblex><pp><*gndr><*numb><@present_...    <*sf>/abrir   \n",
       "4097724               <*sf>/abrir<vblex><cni><*pers><*numb>    <*sf>/abrir   \n",
       "5187118               <*sf>/abrir<vblex><ifi><*pers><*numb>    <*sf>/abrir   \n",
       "11676766              <*sf>/abrir<vblex><fti><*pers><*numb>    <*sf>/abrir   \n",
       "...                                                     ...            ...   \n",
       "11080420        vai/ir<vblex><pri><p3><sg><@future_phrasal>         vai/ir   \n",
       "113367                      vocês/você<prn><tn><p3><mf><pl>     vocês/você   \n",
       "9052892                     vocês/você<prn><tn><p2><mf><pl>     vocês/você   \n",
       "913339           voltar/voltar<vblex><inf><@future_phrasal>  voltar/voltar   \n",
       "5669369                           voltar/voltar<vblex><inf>  voltar/voltar   \n",
       "\n",
       "         word_sf word_clean                                         morph_tags  \n",
       "79107      abrir      abrir          [<*sf>, <vblex>, <pri>, <*pers>, <*numb>]  \n",
       "968967     abrir      abrir  [<*sf>, <vblex>, <pp>, <*gndr>, <*numb>, <@pre...  \n",
       "4097724    abrir      abrir          [<*sf>, <vblex>, <cni>, <*pers>, <*numb>]  \n",
       "5187118    abrir      abrir          [<*sf>, <vblex>, <ifi>, <*pers>, <*numb>]  \n",
       "11676766   abrir      abrir          [<*sf>, <vblex>, <fti>, <*pers>, <*numb>]  \n",
       "...          ...        ...                                                ...  \n",
       "11080420      ir        vai    [<vblex>, <pri>, <p3>, <sg>, <@future_phrasal>]  \n",
       "113367      você      vocês                    [<prn>, <tn>, <p3>, <mf>, <pl>]  \n",
       "9052892     você      vocês                    [<prn>, <tn>, <p2>, <mf>, <pl>]  \n",
       "913339    voltar     voltar                [<vblex>, <inf>, <@future_phrasal>]  \n",
       "5669369   voltar     voltar                                   [<vblex>, <inf>]  \n",
       "\n",
       "[755 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Look at when the same word is duplicated.. why are there different lexeme_ids? Looks like there are different morphological annotation which could be human error but also idk maybe same spelling different meaning like \"I read a book\" can be both present and past tense\n",
    "\n",
    "lexeme_df.loc[lexeme_df.duplicated(subset=[\"word\"], keep=False)].sort_values('word') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c705fd",
   "metadata": {},
   "source": [
    "### Load Dataset B for contextual usage of lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bba118e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(654625, 3)\n",
      "prompt_id        5000\n",
      "translation    653075\n",
      "p               49033\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from src.data.load_data import load_datasetB_txt\n",
    "import os\n",
    "\n",
    "# merge text files into one, dev + test + train\n",
    "dataset_b_dir = \"/Users/andrea/Documents/duolingo_hack/dataset_b/staple-2020/en_pt\"\n",
    "files = [os.path.join(dataset_b_dir, s) for s in os.listdir(dataset_b_dir) if s.endswith(\"gold.txt\")]\n",
    "dataset_b_path = \"/Users/andrea/Documents/duolingo_hack/dataset_b_pt_gold.txt.csv\"\n",
    "\n",
    "with open(dataset_b_path, \"w\") as out:\n",
    "    for f in files:\n",
    "        with open(f, \"r\") as inp:\n",
    "            out.write(inp.read())\n",
    "\n",
    "dataset_b_raw = load_datasetB_txt(dataset_b_path)\n",
    "\n",
    "print(dataset_b_raw.shape)\n",
    "print(dataset_b_raw.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dfb4015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654625/654625 [00:02<00:00, 288947.87it/s]\n",
      "100%|██████████| 10807/10807 [00:00<00:00, 43328.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>embed_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>você está chut&lt;t&gt;a&lt;/t&gt;ndo &lt;t&gt;a&lt;/t&gt; mes&lt;t&gt;a&lt;/t&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abacaxi</td>\n",
       "      <td>bem, aquilo irá ser um &lt;t&gt;abacaxi&lt;/t&gt;.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aberto</td>\n",
       "      <td>o estabelecimento está &lt;t&gt;aberto&lt;/t&gt;?\\no museu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abre</td>\n",
       "      <td>a gente &lt;t&gt;abre&lt;/t&gt; os nossos prédios à comuni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abril</td>\n",
       "      <td>ambos os aniversários são em &lt;t&gt;abril&lt;/t&gt;?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>última</td>\n",
       "      <td>não deixo a pesquisa até a &lt;t&gt;última&lt;/t&gt; hora....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>último</td>\n",
       "      <td>eles viveram na áfrica no &lt;t&gt;último&lt;/t&gt; ano.\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>últimos</td>\n",
       "      <td>nós perdemos nos &lt;t&gt;últimos&lt;/t&gt; momentos do jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>único</td>\n",
       "      <td>dois anos mais tarde, ele venceu por um &lt;t&gt;úni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>útil</td>\n",
       "      <td>fruto fresco é &lt;t&gt;útil&lt;/t&gt; para vossa saúde.\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1639 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        token                                        embed_input\n",
       "0           a  você está chut<t>a</t>ndo <t>a</t> mes<t>a</t>...\n",
       "1     abacaxi             bem, aquilo irá ser um <t>abacaxi</t>.\n",
       "2      aberto  o estabelecimento está <t>aberto</t>?\\no museu...\n",
       "3        abre  a gente <t>abre</t> os nossos prédios à comuni...\n",
       "4       abril         ambos os aniversários são em <t>abril</t>?\n",
       "...       ...                                                ...\n",
       "1634   última  não deixo a pesquisa até a <t>última</t> hora....\n",
       "1635   último  eles viveram na áfrica no <t>último</t> ano.\\n...\n",
       "1636  últimos  nós perdemos nos <t>últimos</t> momentos do jo...\n",
       "1637    único  dois anos mais tarde, ele venceu por um <t>úni...\n",
       "1638     útil  fruto fresco é <t>útil</t> para vossa saúde.\\n...\n",
       "\n",
       "[1639 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.preprocess_lexeme import tokens_from_translation\n",
    "import re\n",
    "\n",
    "sentences_df = dataset_b_raw.copy().assign(\n",
    "    token = lambda x: x.translation.progress_apply(tokens_from_translation)\n",
    ").explode('token')\n",
    "\n",
    "sentences_df.drop_duplicates(subset=['translation', 'token'], inplace=True)\n",
    "sentences_df.sort_values(\"p\", inplace=True)\n",
    "sentences_df.drop_duplicates(subset=['token', 'prompt_id'], keep='last', inplace=True) # keep only most frequently used sentence per translation per token\n",
    "sentences_df = sentences_df.loc[sentences_df.token.isin(set(lexeme_df.word_clean))]\n",
    "sentences_df = sentences_df.groupby(\"token\").tail(10).reset_index(drop=True) # limit to max 10 sentences per token\n",
    "\n",
    "sentences_df['embed_input'] = sentences_df.progress_apply(lambda r: re.sub(f\"({r.token})\", \"<t>\\\\1</t>\", r.translation), axis=1) #tag target token for model\n",
    "sentences_df = sentences_df.groupby('token')['embed_input'].agg(\"\\n\".join).reset_index() ## group sentences into one single input\n",
    "\n",
    "sentences_df#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a00eb584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( set(lexeme_df.word_clean) - set(sentences_df.token) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a90ffe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "token          1992\n",
       "embed_input    1992\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### add in lexemes with no sentences just as it is\n",
    "\n",
    "sentences_df = pd.concat([sentences_df,\n",
    "                          pd.DataFrame({\"token\": list(set(lexeme_df.word_clean) - set(sentences_df.token))}).assign(\n",
    "                            embed_input = lambda x: \"<t>\" + x.token + \"</t>\")\n",
    "                          ])\n",
    "\n",
    "sentences_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1550e604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_df['token']) == len(set(sentences_df.token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba5b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_df.to_parquet(\"../data/tmp/sentences_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723dfa4b",
   "metadata": {},
   "source": [
    "#### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "844662ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## some how tensors can't run in my notebook so i have to put it in a separate script and run from cli\n",
    "\n",
    "# python src/models/LexicalEmbed.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a3779f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.159608</td>\n",
       "      <td>-0.789991</td>\n",
       "      <td>-1.280254</td>\n",
       "      <td>-0.977334</td>\n",
       "      <td>0.201887</td>\n",
       "      <td>1.208807</td>\n",
       "      <td>0.968916</td>\n",
       "      <td>1.482001</td>\n",
       "      <td>0.835401</td>\n",
       "      <td>-0.096745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626819</td>\n",
       "      <td>0.031995</td>\n",
       "      <td>-1.596566</td>\n",
       "      <td>0.052414</td>\n",
       "      <td>-0.401835</td>\n",
       "      <td>-0.444202</td>\n",
       "      <td>0.052495</td>\n",
       "      <td>-0.656819</td>\n",
       "      <td>0.276474</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abacaxi</th>\n",
       "      <td>0.112860</td>\n",
       "      <td>-0.235563</td>\n",
       "      <td>-0.715782</td>\n",
       "      <td>-0.147082</td>\n",
       "      <td>-0.118477</td>\n",
       "      <td>1.047712</td>\n",
       "      <td>1.569708</td>\n",
       "      <td>1.425568</td>\n",
       "      <td>0.915193</td>\n",
       "      <td>-0.496610</td>\n",
       "      <td>...</td>\n",
       "      <td>1.051359</td>\n",
       "      <td>0.696129</td>\n",
       "      <td>-0.768505</td>\n",
       "      <td>-0.528718</td>\n",
       "      <td>-2.356428</td>\n",
       "      <td>-0.940612</td>\n",
       "      <td>0.085490</td>\n",
       "      <td>-1.276624</td>\n",
       "      <td>-0.594671</td>\n",
       "      <td>0.491628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abacaxis</th>\n",
       "      <td>0.104341</td>\n",
       "      <td>-0.850763</td>\n",
       "      <td>0.733975</td>\n",
       "      <td>0.536416</td>\n",
       "      <td>-0.168587</td>\n",
       "      <td>0.349298</td>\n",
       "      <td>1.075039</td>\n",
       "      <td>1.731350</td>\n",
       "      <td>-0.362329</td>\n",
       "      <td>0.992692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.446859</td>\n",
       "      <td>0.062616</td>\n",
       "      <td>-0.588157</td>\n",
       "      <td>0.616257</td>\n",
       "      <td>-1.667086</td>\n",
       "      <td>1.415089</td>\n",
       "      <td>-0.884691</td>\n",
       "      <td>-0.613696</td>\n",
       "      <td>-0.322179</td>\n",
       "      <td>0.046296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abelha</th>\n",
       "      <td>-0.026599</td>\n",
       "      <td>-0.907398</td>\n",
       "      <td>-1.250067</td>\n",
       "      <td>-1.786331</td>\n",
       "      <td>-0.295168</td>\n",
       "      <td>1.354755</td>\n",
       "      <td>0.893484</td>\n",
       "      <td>1.579531</td>\n",
       "      <td>0.631786</td>\n",
       "      <td>-0.815755</td>\n",
       "      <td>...</td>\n",
       "      <td>1.308146</td>\n",
       "      <td>0.419317</td>\n",
       "      <td>-1.364628</td>\n",
       "      <td>0.393218</td>\n",
       "      <td>-0.948960</td>\n",
       "      <td>0.736760</td>\n",
       "      <td>0.708868</td>\n",
       "      <td>-1.746192</td>\n",
       "      <td>-0.158883</td>\n",
       "      <td>0.874223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aberto</th>\n",
       "      <td>-0.249868</td>\n",
       "      <td>-1.083624</td>\n",
       "      <td>-0.607750</td>\n",
       "      <td>-0.709888</td>\n",
       "      <td>0.272246</td>\n",
       "      <td>0.935445</td>\n",
       "      <td>-0.264850</td>\n",
       "      <td>1.032278</td>\n",
       "      <td>0.645995</td>\n",
       "      <td>-1.262559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441675</td>\n",
       "      <td>-1.242004</td>\n",
       "      <td>-0.728554</td>\n",
       "      <td>0.676841</td>\n",
       "      <td>0.136742</td>\n",
       "      <td>0.905638</td>\n",
       "      <td>1.376844</td>\n",
       "      <td>0.309301</td>\n",
       "      <td>2.045292</td>\n",
       "      <td>0.682605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abre</th>\n",
       "      <td>-1.196108</td>\n",
       "      <td>-0.452264</td>\n",
       "      <td>-1.141945</td>\n",
       "      <td>-1.126796</td>\n",
       "      <td>0.123103</td>\n",
       "      <td>1.630528</td>\n",
       "      <td>0.358479</td>\n",
       "      <td>0.949989</td>\n",
       "      <td>0.166656</td>\n",
       "      <td>-0.469382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913266</td>\n",
       "      <td>-1.331908</td>\n",
       "      <td>-1.026353</td>\n",
       "      <td>-0.047394</td>\n",
       "      <td>0.474761</td>\n",
       "      <td>0.989215</td>\n",
       "      <td>0.839568</td>\n",
       "      <td>0.237909</td>\n",
       "      <td>0.977563</td>\n",
       "      <td>1.516168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abril</th>\n",
       "      <td>0.607180</td>\n",
       "      <td>-0.385094</td>\n",
       "      <td>-2.137380</td>\n",
       "      <td>-0.101805</td>\n",
       "      <td>0.418300</td>\n",
       "      <td>-0.987933</td>\n",
       "      <td>0.606662</td>\n",
       "      <td>1.320576</td>\n",
       "      <td>1.221782</td>\n",
       "      <td>0.231729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189767</td>\n",
       "      <td>0.067621</td>\n",
       "      <td>0.528885</td>\n",
       "      <td>-0.788262</td>\n",
       "      <td>0.063432</td>\n",
       "      <td>-0.264308</td>\n",
       "      <td>0.539284</td>\n",
       "      <td>-0.391675</td>\n",
       "      <td>-0.717743</td>\n",
       "      <td>0.732163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abrir</th>\n",
       "      <td>-1.464728</td>\n",
       "      <td>-0.388119</td>\n",
       "      <td>-0.922581</td>\n",
       "      <td>-1.405381</td>\n",
       "      <td>-0.467532</td>\n",
       "      <td>1.567218</td>\n",
       "      <td>0.568368</td>\n",
       "      <td>0.653508</td>\n",
       "      <td>0.144286</td>\n",
       "      <td>-0.811723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364821</td>\n",
       "      <td>-1.542726</td>\n",
       "      <td>-0.662923</td>\n",
       "      <td>-0.142513</td>\n",
       "      <td>0.240180</td>\n",
       "      <td>1.016949</td>\n",
       "      <td>0.573348</td>\n",
       "      <td>0.411292</td>\n",
       "      <td>1.421060</td>\n",
       "      <td>0.972885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolutamente</th>\n",
       "      <td>0.159063</td>\n",
       "      <td>-0.590736</td>\n",
       "      <td>-0.779604</td>\n",
       "      <td>-0.515869</td>\n",
       "      <td>-0.216373</td>\n",
       "      <td>1.032541</td>\n",
       "      <td>1.422664</td>\n",
       "      <td>-0.953403</td>\n",
       "      <td>1.104002</td>\n",
       "      <td>-0.622430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735080</td>\n",
       "      <td>0.263916</td>\n",
       "      <td>-1.348840</td>\n",
       "      <td>0.063932</td>\n",
       "      <td>-0.998149</td>\n",
       "      <td>2.046380</td>\n",
       "      <td>0.422090</td>\n",
       "      <td>-0.165625</td>\n",
       "      <td>-1.011416</td>\n",
       "      <td>1.905915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acabar</th>\n",
       "      <td>0.587410</td>\n",
       "      <td>0.318691</td>\n",
       "      <td>-0.419808</td>\n",
       "      <td>-0.828312</td>\n",
       "      <td>-0.381332</td>\n",
       "      <td>1.388464</td>\n",
       "      <td>-0.653525</td>\n",
       "      <td>2.259298</td>\n",
       "      <td>0.718048</td>\n",
       "      <td>0.240456</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349814</td>\n",
       "      <td>-1.352566</td>\n",
       "      <td>-0.999245</td>\n",
       "      <td>0.813829</td>\n",
       "      <td>-2.317532</td>\n",
       "      <td>-0.224350</td>\n",
       "      <td>1.425098</td>\n",
       "      <td>-0.667375</td>\n",
       "      <td>0.273414</td>\n",
       "      <td>-0.031830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4         5     \\\n",
       "token                                                                       \n",
       "a              0.159608 -0.789991 -1.280254 -0.977334  0.201887  1.208807   \n",
       "abacaxi        0.112860 -0.235563 -0.715782 -0.147082 -0.118477  1.047712   \n",
       "abacaxis       0.104341 -0.850763  0.733975  0.536416 -0.168587  0.349298   \n",
       "abelha        -0.026599 -0.907398 -1.250067 -1.786331 -0.295168  1.354755   \n",
       "aberto        -0.249868 -1.083624 -0.607750 -0.709888  0.272246  0.935445   \n",
       "abre          -1.196108 -0.452264 -1.141945 -1.126796  0.123103  1.630528   \n",
       "abril          0.607180 -0.385094 -2.137380 -0.101805  0.418300 -0.987933   \n",
       "abrir         -1.464728 -0.388119 -0.922581 -1.405381 -0.467532  1.567218   \n",
       "absolutamente  0.159063 -0.590736 -0.779604 -0.515869 -0.216373  1.032541   \n",
       "acabar         0.587410  0.318691 -0.419808 -0.828312 -0.381332  1.388464   \n",
       "\n",
       "                   6         7         8         9     ...      1014  \\\n",
       "token                                                  ...             \n",
       "a              0.968916  1.482001  0.835401 -0.096745  ...  0.626819   \n",
       "abacaxi        1.569708  1.425568  0.915193 -0.496610  ...  1.051359   \n",
       "abacaxis       1.075039  1.731350 -0.362329  0.992692  ... -0.446859   \n",
       "abelha         0.893484  1.579531  0.631786 -0.815755  ...  1.308146   \n",
       "aberto        -0.264850  1.032278  0.645995 -1.262559  ...  0.441675   \n",
       "abre           0.358479  0.949989  0.166656 -0.469382  ...  0.913266   \n",
       "abril          0.606662  1.320576  1.221782  0.231729  ...  0.189767   \n",
       "abrir          0.568368  0.653508  0.144286 -0.811723  ...  0.364821   \n",
       "absolutamente  1.422664 -0.953403  1.104002 -0.622430  ...  0.735080   \n",
       "acabar        -0.653525  2.259298  0.718048  0.240456  ... -0.349814   \n",
       "\n",
       "                   1015      1016      1017      1018      1019      1020  \\\n",
       "token                                                                       \n",
       "a              0.031995 -1.596566  0.052414 -0.401835 -0.444202  0.052495   \n",
       "abacaxi        0.696129 -0.768505 -0.528718 -2.356428 -0.940612  0.085490   \n",
       "abacaxis       0.062616 -0.588157  0.616257 -1.667086  1.415089 -0.884691   \n",
       "abelha         0.419317 -1.364628  0.393218 -0.948960  0.736760  0.708868   \n",
       "aberto        -1.242004 -0.728554  0.676841  0.136742  0.905638  1.376844   \n",
       "abre          -1.331908 -1.026353 -0.047394  0.474761  0.989215  0.839568   \n",
       "abril          0.067621  0.528885 -0.788262  0.063432 -0.264308  0.539284   \n",
       "abrir         -1.542726 -0.662923 -0.142513  0.240180  1.016949  0.573348   \n",
       "absolutamente  0.263916 -1.348840  0.063932 -0.998149  2.046380  0.422090   \n",
       "acabar        -1.352566 -0.999245  0.813829 -2.317532 -0.224350  1.425098   \n",
       "\n",
       "                   1021      1022      1023  \n",
       "token                                        \n",
       "a             -0.656819  0.276474  0.807692  \n",
       "abacaxi       -1.276624 -0.594671  0.491628  \n",
       "abacaxis      -0.613696 -0.322179  0.046296  \n",
       "abelha        -1.746192 -0.158883  0.874223  \n",
       "aberto         0.309301  2.045292  0.682605  \n",
       "abre           0.237909  0.977563  1.516168  \n",
       "abril         -0.391675 -0.717743  0.732163  \n",
       "abrir          0.411292  1.421060  0.972885  \n",
       "absolutamente -0.165625 -1.011416  1.905915  \n",
       "acabar        -0.667375  0.273414 -0.031830  \n",
       "\n",
       "[10 rows x 1024 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem_embeddings = pd.read_parquet(\"../data/tmp/embeddings.parquet\")\n",
    "sem_embeddings.index = sentences_df['token']\n",
    "sem_embeddings.sort_index(inplace=True)\n",
    "sem_embeddings.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57dda3",
   "metadata": {},
   "source": [
    "#### Add Morphological Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1abc5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tags found: 61\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;*gndr&gt;</th>\n",
       "      <th>&lt;*numb&gt;</th>\n",
       "      <th>&lt;*pers&gt;</th>\n",
       "      <th>&lt;*sf&gt;</th>\n",
       "      <th>&lt;@cond_perfect&gt;</th>\n",
       "      <th>&lt;@future_perfect&gt;</th>\n",
       "      <th>&lt;@future_phrasal&gt;</th>\n",
       "      <th>&lt;@past_perfect&gt;</th>\n",
       "      <th>&lt;@present_perfect&gt;</th>\n",
       "      <th>&lt;@subjunctive_pluperfect&gt;</th>\n",
       "      <th>...</th>\n",
       "      <th>&lt;rel&gt;</th>\n",
       "      <th>&lt;sg&gt;</th>\n",
       "      <th>&lt;sp&gt;</th>\n",
       "      <th>&lt;tn&gt;</th>\n",
       "      <th>&lt;vbhaver&gt;</th>\n",
       "      <th>&lt;vblex&gt;</th>\n",
       "      <th>&lt;vbmod&gt;</th>\n",
       "      <th>&lt;vbser&gt;</th>\n",
       "      <th>lexeme_id</th>\n",
       "      <th>word_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57408f89412af98111a2f87c0ab41b22</td>\n",
       "      <td>tu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8414835cb39e4315146a59fefdd6d1c6</td>\n",
       "      <td>tem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ecc3feb8e53ce936cef181dd54e7aaca</td>\n",
       "      <td>temos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8d28ba0fa188f1847571467189846dda</td>\n",
       "      <td>tua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4b3613233b3fede2e3e92ac2ef752bf6</td>\n",
       "      <td>leão</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    <*gndr>  <*numb>  <*pers>  <*sf>  <@cond_perfect>  <@future_perfect>  \\\n",
       "64        0        0        0      0                0                  0   \n",
       "65        0        0        0      0                0                  0   \n",
       "66        0        0        0      0                0                  0   \n",
       "67        0        0        0      0                0                  0   \n",
       "68        0        0        0      0                0                  0   \n",
       "\n",
       "    <@future_phrasal>  <@past_perfect>  <@present_perfect>  \\\n",
       "64                  0                0                   0   \n",
       "65                  0                0                   0   \n",
       "66                  0                0                   0   \n",
       "67                  0                0                   0   \n",
       "68                  0                0                   0   \n",
       "\n",
       "    <@subjunctive_pluperfect>  ...  <rel>  <sg>  <sp>  <tn>  <vbhaver>  \\\n",
       "64                          0  ...      0     1     0     1          0   \n",
       "65                          0  ...      0     1     0     0          0   \n",
       "66                          0  ...      0     0     0     0          0   \n",
       "67                          0  ...      0     1     0     0          0   \n",
       "68                          0  ...      0     1     0     0          0   \n",
       "\n",
       "    <vblex>  <vbmod>  <vbser>                         lexeme_id  word_clean  \n",
       "64        0        0        0  57408f89412af98111a2f87c0ab41b22          tu  \n",
       "65        1        0        0  8414835cb39e4315146a59fefdd6d1c6         tem  \n",
       "66        1        0        0  ecc3feb8e53ce936cef181dd54e7aaca       temos  \n",
       "67        0        0        0  8d28ba0fa188f1847571467189846dda         tua  \n",
       "68        0        0        0  4b3613233b3fede2e3e92ac2ef752bf6        leão  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Initialize the binarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Transform the 'morph_tags' column into a binary matrix\n",
    "# This creates a NumPy array where 1 means the tag is present, 0 otherwise\n",
    "morph_encoded = mlb.fit_transform(lexeme_df['morph_tags'])\n",
    "\n",
    "# Create a DataFrame from the encoded tags to see the labels\n",
    "morph_df = pd.DataFrame(morph_encoded, columns=mlb.classes_, index=lexeme_df.lexeme_id).merge(lexeme_df[['lexeme_id', 'word_clean']], left_index=True, right_on='lexeme_id')\n",
    "\n",
    "# Preview the result\n",
    "print(f\"Total unique tags found: {len(mlb.classes_)}\")\n",
    "morph_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fac02455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;*gndr&gt;</th>\n",
       "      <th>&lt;*numb&gt;</th>\n",
       "      <th>&lt;*pers&gt;</th>\n",
       "      <th>&lt;*sf&gt;</th>\n",
       "      <th>&lt;@cond_perfect&gt;</th>\n",
       "      <th>&lt;@future_perfect&gt;</th>\n",
       "      <th>&lt;@future_phrasal&gt;</th>\n",
       "      <th>&lt;@past_perfect&gt;</th>\n",
       "      <th>&lt;@present_perfect&gt;</th>\n",
       "      <th>&lt;@subjunctive_pluperfect&gt;</th>\n",
       "      <th>...</th>\n",
       "      <th>f1014</th>\n",
       "      <th>f1015</th>\n",
       "      <th>f1016</th>\n",
       "      <th>f1017</th>\n",
       "      <th>f1018</th>\n",
       "      <th>f1019</th>\n",
       "      <th>f1020</th>\n",
       "      <th>f1021</th>\n",
       "      <th>f1022</th>\n",
       "      <th>f1023</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexeme_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57408f89412af98111a2f87c0ab41b22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151807</td>\n",
       "      <td>-0.992755</td>\n",
       "      <td>-1.395375</td>\n",
       "      <td>-0.450079</td>\n",
       "      <td>-0.641103</td>\n",
       "      <td>0.593024</td>\n",
       "      <td>1.284555</td>\n",
       "      <td>-0.836021</td>\n",
       "      <td>0.509478</td>\n",
       "      <td>1.917565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8414835cb39e4315146a59fefdd6d1c6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705593</td>\n",
       "      <td>-0.755042</td>\n",
       "      <td>-1.906103</td>\n",
       "      <td>0.615944</td>\n",
       "      <td>-1.583506</td>\n",
       "      <td>1.072103</td>\n",
       "      <td>0.980725</td>\n",
       "      <td>-0.062932</td>\n",
       "      <td>-0.980165</td>\n",
       "      <td>-0.344127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecc3feb8e53ce936cef181dd54e7aaca</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.711553</td>\n",
       "      <td>-1.145459</td>\n",
       "      <td>-1.590241</td>\n",
       "      <td>0.747374</td>\n",
       "      <td>-1.808892</td>\n",
       "      <td>1.593481</td>\n",
       "      <td>1.002151</td>\n",
       "      <td>0.299135</td>\n",
       "      <td>-1.026802</td>\n",
       "      <td>-0.642223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8d28ba0fa188f1847571467189846dda</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175844</td>\n",
       "      <td>0.845173</td>\n",
       "      <td>-1.385550</td>\n",
       "      <td>-0.825235</td>\n",
       "      <td>0.208458</td>\n",
       "      <td>1.289223</td>\n",
       "      <td>0.629938</td>\n",
       "      <td>-1.519976</td>\n",
       "      <td>-0.514366</td>\n",
       "      <td>-0.091442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4b3613233b3fede2e3e92ac2ef752bf6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.142239</td>\n",
       "      <td>-0.429955</td>\n",
       "      <td>-0.531885</td>\n",
       "      <td>-0.165027</td>\n",
       "      <td>-0.797985</td>\n",
       "      <td>-0.302957</td>\n",
       "      <td>-0.043292</td>\n",
       "      <td>-1.206904</td>\n",
       "      <td>-0.114630</td>\n",
       "      <td>0.583535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1085 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  <*gndr>  <*numb>  <*pers>  <*sf>  \\\n",
       "lexeme_id                                                            \n",
       "57408f89412af98111a2f87c0ab41b22        0        0        0      0   \n",
       "8414835cb39e4315146a59fefdd6d1c6        0        0        0      0   \n",
       "ecc3feb8e53ce936cef181dd54e7aaca        0        0        0      0   \n",
       "8d28ba0fa188f1847571467189846dda        0        0        0      0   \n",
       "4b3613233b3fede2e3e92ac2ef752bf6        0        0        0      0   \n",
       "\n",
       "                                  <@cond_perfect>  <@future_perfect>  \\\n",
       "lexeme_id                                                              \n",
       "57408f89412af98111a2f87c0ab41b22                0                  0   \n",
       "8414835cb39e4315146a59fefdd6d1c6                0                  0   \n",
       "ecc3feb8e53ce936cef181dd54e7aaca                0                  0   \n",
       "8d28ba0fa188f1847571467189846dda                0                  0   \n",
       "4b3613233b3fede2e3e92ac2ef752bf6                0                  0   \n",
       "\n",
       "                                  <@future_phrasal>  <@past_perfect>  \\\n",
       "lexeme_id                                                              \n",
       "57408f89412af98111a2f87c0ab41b22                  0                0   \n",
       "8414835cb39e4315146a59fefdd6d1c6                  0                0   \n",
       "ecc3feb8e53ce936cef181dd54e7aaca                  0                0   \n",
       "8d28ba0fa188f1847571467189846dda                  0                0   \n",
       "4b3613233b3fede2e3e92ac2ef752bf6                  0                0   \n",
       "\n",
       "                                  <@present_perfect>  \\\n",
       "lexeme_id                                              \n",
       "57408f89412af98111a2f87c0ab41b22                   0   \n",
       "8414835cb39e4315146a59fefdd6d1c6                   0   \n",
       "ecc3feb8e53ce936cef181dd54e7aaca                   0   \n",
       "8d28ba0fa188f1847571467189846dda                   0   \n",
       "4b3613233b3fede2e3e92ac2ef752bf6                   0   \n",
       "\n",
       "                                  <@subjunctive_pluperfect>  ...     f1014  \\\n",
       "lexeme_id                                                    ...             \n",
       "57408f89412af98111a2f87c0ab41b22                          0  ...  0.151807   \n",
       "8414835cb39e4315146a59fefdd6d1c6                          0  ...  0.705593   \n",
       "ecc3feb8e53ce936cef181dd54e7aaca                          0  ...  0.711553   \n",
       "8d28ba0fa188f1847571467189846dda                          0  ...  0.175844   \n",
       "4b3613233b3fede2e3e92ac2ef752bf6                          0  ...  1.142239   \n",
       "\n",
       "                                     f1015     f1016     f1017     f1018  \\\n",
       "lexeme_id                                                                  \n",
       "57408f89412af98111a2f87c0ab41b22 -0.992755 -1.395375 -0.450079 -0.641103   \n",
       "8414835cb39e4315146a59fefdd6d1c6 -0.755042 -1.906103  0.615944 -1.583506   \n",
       "ecc3feb8e53ce936cef181dd54e7aaca -1.145459 -1.590241  0.747374 -1.808892   \n",
       "8d28ba0fa188f1847571467189846dda  0.845173 -1.385550 -0.825235  0.208458   \n",
       "4b3613233b3fede2e3e92ac2ef752bf6 -0.429955 -0.531885 -0.165027 -0.797985   \n",
       "\n",
       "                                     f1019     f1020     f1021     f1022  \\\n",
       "lexeme_id                                                                  \n",
       "57408f89412af98111a2f87c0ab41b22  0.593024  1.284555 -0.836021  0.509478   \n",
       "8414835cb39e4315146a59fefdd6d1c6  1.072103  0.980725 -0.062932 -0.980165   \n",
       "ecc3feb8e53ce936cef181dd54e7aaca  1.593481  1.002151  0.299135 -1.026802   \n",
       "8d28ba0fa188f1847571467189846dda  1.289223  0.629938 -1.519976 -0.514366   \n",
       "4b3613233b3fede2e3e92ac2ef752bf6 -0.302957 -0.043292 -1.206904 -0.114630   \n",
       "\n",
       "                                     f1023  \n",
       "lexeme_id                                   \n",
       "57408f89412af98111a2f87c0ab41b22  1.917565  \n",
       "8414835cb39e4315146a59fefdd6d1c6 -0.344127  \n",
       "ecc3feb8e53ce936cef181dd54e7aaca -0.642223  \n",
       "8d28ba0fa188f1847571467189846dda -0.091442  \n",
       "4b3613233b3fede2e3e92ac2ef752bf6  0.583535  \n",
       "\n",
       "[5 rows x 1085 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexeme_feat_df = morph_df.merge(sem_embeddings.rename(columns=lambda x: \"f\"+str(x)), how='left', left_on='word_clean', right_index=True,\n",
    "                                ).drop(columns=['word_clean']\n",
    "                                ).set_index(\"lexeme_id\")\n",
    "lexeme_feat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff8c5d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<*gndr>               2\n",
       "<*numb>               2\n",
       "<*pers>               2\n",
       "<*sf>                 2\n",
       "<@cond_perfect>       2\n",
       "                   ... \n",
       "f1020              1991\n",
       "f1021              1992\n",
       "f1022              1992\n",
       "f1023              1992\n",
       "cluster             223\n",
       "Length: 1086, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexeme_feat_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "002546f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40341377, -1.26461904, -0.47263214, ..., -0.35747443,\n",
       "         0.58911243,  1.84960161],\n",
       "       [-0.40341377, -1.26461904, -0.47263214, ...,  0.67352118,\n",
       "        -1.11654319, -0.95096521],\n",
       "       [-0.40341377, -1.26461904, -0.47263214, ...,  1.15637612,\n",
       "        -1.16994337, -1.32008707],\n",
       "       ...,\n",
       "       [-0.40341377, -1.26461904, -0.47263214, ...,  2.66162436,\n",
       "         0.32369183,  0.27212726],\n",
       "       [-0.40341377, -1.26461904, -0.47263214, ...,  0.42043112,\n",
       "         0.99759677,  1.461434  ],\n",
       "       [-0.40341377, -1.26461904, -0.47263214, ..., -0.38164205,\n",
       "         0.3426035 ,  1.23227425]], shape=(2815, 1085))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "lexeme_feat_df_scaled = scaler.fit_transform(lexeme_feat_df)\n",
    "lexeme_feat_df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67ad18",
   "metadata": {},
   "source": [
    "### Cluster Lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6d5cd16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/duohack/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "import umap\n",
    "\n",
    "# 1. Reduce dimensions while preserving local structure\n",
    "# This makes density-based clustering much more effective\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=15, \n",
    "    n_components=60, \n",
    "    metric='cosine', # Use cosine since we're dealing with embeddings\n",
    "    random_state=12345\n",
    ")\n",
    "u_embeddings = reducer.fit_transform(lexeme_feat_df_scaled)\n",
    "\n",
    "# 2. Initialize and fit HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=15,       # Smallest group you'd consider a 'cluster'\n",
    "    min_samples=1,            # Lower values = more clusters, fewer 'noise' points\n",
    "    metric='euclidean',       # Use euclidean on the UMAP output\n",
    "    cluster_selection_method='eom' # 'Excess of Mass' finds stable clusters\n",
    ")\n",
    "\n",
    "cluster_labels = clusterer.fit_predict(u_embeddings)\n",
    "\n",
    "# 3. Add results back to your dataframe\n",
    "lexeme_feat_df['cluster'] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "812a4487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     82.000000\n",
       "mean      34.329268\n",
       "std       44.835517\n",
       "min       15.000000\n",
       "25%       18.250000\n",
       "50%       25.000000\n",
       "75%       35.500000\n",
       "max      402.000000\n",
       "Name: lexeme_id, dtype: float64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexeme_feat_df.reset_index().groupby('cluster')['lexeme_id'].nunique().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "79dcb41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexeme_feat_df.to_parquet(\"../data/lexeme_embed_cluster_results.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5dd1b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexeme_cluster_results = lexeme_feat_df[['cluster']].merge(lexeme_df.set_index(\"lexeme_id\")['word'], 'left', left_index=True, right_index=True)\n",
    "\n",
    "lexeme_cluster_results.to_csv(\"../data/lexeme_cluster_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "87c280f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexeme_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3b90f64dfa33918b221e0aabd249dc75</th>\n",
       "      <td>35</td>\n",
       "      <td>quando/quando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826a902bc42b3ac85918087164174ab0</th>\n",
       "      <td>35</td>\n",
       "      <td>desde/desde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afcb0785116e603e09dc46ceed0be5ce</th>\n",
       "      <td>35</td>\n",
       "      <td>enquanto/enquanto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e82fa7904e0027596d88bcfc652415d7</th>\n",
       "      <td>35</td>\n",
       "      <td>logo/logo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f3ac9fa9e991238c93688ca47a12edd2</th>\n",
       "      <td>35</td>\n",
       "      <td>então/então</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2ec34df87a3756d67e9fb65f3d51c164</th>\n",
       "      <td>35</td>\n",
       "      <td>sempre/sempre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4d2509dacbcc90cddac88cb840fc1dd3</th>\n",
       "      <td>35</td>\n",
       "      <td>agora/agora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80dab43ce8e8a5c1296cc45209c74283</th>\n",
       "      <td>35</td>\n",
       "      <td>já/já</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9e0893bcfc6d73adb82afc03a51c1451</th>\n",
       "      <td>35</td>\n",
       "      <td>durante/durante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c198fdbbf74e825b205af212b2347253</th>\n",
       "      <td>35</td>\n",
       "      <td>às/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475d3fae09f57455651d374bace047f6</th>\n",
       "      <td>35</td>\n",
       "      <td>enquanto/enquanto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cacdce29f9923de92f277b48bd833ac4</th>\n",
       "      <td>35</td>\n",
       "      <td>atrás/atrás</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3ad2aa1617a9caf749d41bee61867ff4</th>\n",
       "      <td>35</td>\n",
       "      <td>então/então</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88ddaf57ab676fa397b74df0ff3bdf4f</th>\n",
       "      <td>35</td>\n",
       "      <td>&lt;*sf&gt;/momento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303266d9b3b2326cdc182ea67d3dd0cc</th>\n",
       "      <td>35</td>\n",
       "      <td>&lt;*sf&gt;/vez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709e5de5587a66f8326de386d1a256b0</th>\n",
       "      <td>35</td>\n",
       "      <td>&lt;*sf&gt;/futuro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91883f1e1365386ef90817b4c9755477</th>\n",
       "      <td>35</td>\n",
       "      <td>normalmente/normalmente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3e2a1c334ee942fedd9a6f6acf4f9950</th>\n",
       "      <td>35</td>\n",
       "      <td>ainda/ainda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367c6f5e25e27f42f511622738487c40</th>\n",
       "      <td>35</td>\n",
       "      <td>logo/logo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86ab6987e00469b602b133a325160254</th>\n",
       "      <td>35</td>\n",
       "      <td>recentemente/recentemente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d9f648c7b7018b477d0bb757e697a77c</th>\n",
       "      <td>35</td>\n",
       "      <td>antes/antes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dda59c1e6b8fb94f45061d9a5b453a78</th>\n",
       "      <td>35</td>\n",
       "      <td>depois/depois</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6080c8b81bba99d3c592df66cbd47efb</th>\n",
       "      <td>35</td>\n",
       "      <td>&lt;*sf&gt;/último</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4cbf04efacfc5ccea49565b093870c30</th>\n",
       "      <td>35</td>\n",
       "      <td>geralmente/geralmente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2b413a072143d0546934c6511681ad8c</th>\n",
       "      <td>35</td>\n",
       "      <td>nunca/nunca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7fb5ec1bf6f73b7594f217467c73d2bc</th>\n",
       "      <td>35</td>\n",
       "      <td>novamente/novamente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91acbc78f7a09e5b8ba6ebfc9ac6ae02</th>\n",
       "      <td>35</td>\n",
       "      <td>finalmente/finalmente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68496e5ee3735a07422d35e2edd57855</th>\n",
       "      <td>35</td>\n",
       "      <td>imediatamente/imediatamente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2d83a759bee8e5dd2a910823ab215a9</th>\n",
       "      <td>35</td>\n",
       "      <td>anteriormente/anteriormente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963ff8d7da2b3accc70d50ba3fd2f94d</th>\n",
       "      <td>35</td>\n",
       "      <td>&lt;*sf&gt;/instante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e213300a44ff6169772db6654a483f83</th>\n",
       "      <td>35</td>\n",
       "      <td>última/último</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70e7507688f65eb203d446b7cea9abef</th>\n",
       "      <td>35</td>\n",
       "      <td>instante/instante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a0d4cf44cdc6b5d6b565999b6abde10b</th>\n",
       "      <td>35</td>\n",
       "      <td>vez/vez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e916475d8b00132a9a69f30683ecd088</th>\n",
       "      <td>35</td>\n",
       "      <td>atualmente/atualmente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63b2578b65da624a3cf6e13b18e07c7f</th>\n",
       "      <td>35</td>\n",
       "      <td>&lt;*sf&gt;/próximo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f523826b76a05f8d76f304095bc17def</th>\n",
       "      <td>35</td>\n",
       "      <td>&lt;*sf&gt;/sessão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74444dd8d01dc702dfb704ba95681f6a</th>\n",
       "      <td>35</td>\n",
       "      <td>próxima/próximo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b1f8777c7b6afb79abd04be1eba77263</th>\n",
       "      <td>35</td>\n",
       "      <td>recém/recém</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6014413ad4ccbd8681d62b934b682ab9</th>\n",
       "      <td>35</td>\n",
       "      <td>últimos/último</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  cluster                         word\n",
       "lexeme_id                                                             \n",
       "3b90f64dfa33918b221e0aabd249dc75       35                quando/quando\n",
       "826a902bc42b3ac85918087164174ab0       35                  desde/desde\n",
       "afcb0785116e603e09dc46ceed0be5ce       35            enquanto/enquanto\n",
       "e82fa7904e0027596d88bcfc652415d7       35                    logo/logo\n",
       "f3ac9fa9e991238c93688ca47a12edd2       35                  então/então\n",
       "2ec34df87a3756d67e9fb65f3d51c164       35                sempre/sempre\n",
       "4d2509dacbcc90cddac88cb840fc1dd3       35                  agora/agora\n",
       "80dab43ce8e8a5c1296cc45209c74283       35                        já/já\n",
       "9e0893bcfc6d73adb82afc03a51c1451       35              durante/durante\n",
       "c198fdbbf74e825b205af212b2347253       35                         às/a\n",
       "475d3fae09f57455651d374bace047f6       35            enquanto/enquanto\n",
       "cacdce29f9923de92f277b48bd833ac4       35                  atrás/atrás\n",
       "3ad2aa1617a9caf749d41bee61867ff4       35                  então/então\n",
       "88ddaf57ab676fa397b74df0ff3bdf4f       35                <*sf>/momento\n",
       "303266d9b3b2326cdc182ea67d3dd0cc       35                    <*sf>/vez\n",
       "709e5de5587a66f8326de386d1a256b0       35                 <*sf>/futuro\n",
       "91883f1e1365386ef90817b4c9755477       35      normalmente/normalmente\n",
       "3e2a1c334ee942fedd9a6f6acf4f9950       35                  ainda/ainda\n",
       "367c6f5e25e27f42f511622738487c40       35                    logo/logo\n",
       "86ab6987e00469b602b133a325160254       35    recentemente/recentemente\n",
       "d9f648c7b7018b477d0bb757e697a77c       35                  antes/antes\n",
       "dda59c1e6b8fb94f45061d9a5b453a78       35                depois/depois\n",
       "6080c8b81bba99d3c592df66cbd47efb       35                 <*sf>/último\n",
       "4cbf04efacfc5ccea49565b093870c30       35        geralmente/geralmente\n",
       "2b413a072143d0546934c6511681ad8c       35                  nunca/nunca\n",
       "7fb5ec1bf6f73b7594f217467c73d2bc       35          novamente/novamente\n",
       "91acbc78f7a09e5b8ba6ebfc9ac6ae02       35        finalmente/finalmente\n",
       "68496e5ee3735a07422d35e2edd57855       35  imediatamente/imediatamente\n",
       "f2d83a759bee8e5dd2a910823ab215a9       35  anteriormente/anteriormente\n",
       "963ff8d7da2b3accc70d50ba3fd2f94d       35               <*sf>/instante\n",
       "e213300a44ff6169772db6654a483f83       35                última/último\n",
       "70e7507688f65eb203d446b7cea9abef       35            instante/instante\n",
       "a0d4cf44cdc6b5d6b565999b6abde10b       35                      vez/vez\n",
       "e916475d8b00132a9a69f30683ecd088       35        atualmente/atualmente\n",
       "63b2578b65da624a3cf6e13b18e07c7f       35                <*sf>/próximo\n",
       "f523826b76a05f8d76f304095bc17def       35                 <*sf>/sessão\n",
       "74444dd8d01dc702dfb704ba95681f6a       35              próxima/próximo\n",
       "b1f8777c7b6afb79abd04be1eba77263       35                  recém/recém\n",
       "6014413ad4ccbd8681d62b934b682ab9       35               últimos/último"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexeme_cluster_results.loc[lexeme_cluster_results.cluster==35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d6095e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexeme_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6e39fa977508d7c2e5990cfddc80b2f7</th>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;*sf&gt;/fantasia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3a898602c268f23d8acbeadacc9138be</th>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;*sf&gt;/roupa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a650b5f58f811a5360c1c94772525a83</th>\n",
       "      <td>-1</td>\n",
       "      <td>gosta/gostar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0dd6934ef0f42a5151850fa8ca018c2c</th>\n",
       "      <td>-1</td>\n",
       "      <td>és/ser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b2e1f3421eb5216ac144a5714dbf7d6b</th>\n",
       "      <td>-1</td>\n",
       "      <td>lê/ler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2aacad8db68efca9433ded0e0169687</th>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;*sf&gt;/pista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8c8327dafa63dfd709efbd75117a3312</th>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;*sf&gt;/moda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61c3e481fa09977511996a7fc728c00b</th>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;*sf&gt;/linha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289ec4ae458f4741f9401eb234dd29a3</th>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;*sf&gt;/acontecer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886cae538de4af8b17d0641ffc469d5a</th>\n",
       "      <td>-1</td>\n",
       "      <td>terminou/terminar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>402 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  cluster               word\n",
       "lexeme_id                                                   \n",
       "6e39fa977508d7c2e5990cfddc80b2f7       -1     <*sf>/fantasia\n",
       "3a898602c268f23d8acbeadacc9138be       -1        <*sf>/roupa\n",
       "a650b5f58f811a5360c1c94772525a83       -1       gosta/gostar\n",
       "0dd6934ef0f42a5151850fa8ca018c2c       -1             és/ser\n",
       "b2e1f3421eb5216ac144a5714dbf7d6b       -1             lê/ler\n",
       "...                                   ...                ...\n",
       "c2aacad8db68efca9433ded0e0169687       -1        <*sf>/pista\n",
       "8c8327dafa63dfd709efbd75117a3312       -1         <*sf>/moda\n",
       "61c3e481fa09977511996a7fc728c00b       -1        <*sf>/linha\n",
       "289ec4ae458f4741f9401eb234dd29a3       -1    <*sf>/acontecer\n",
       "886cae538de4af8b17d0641ffc469d5a       -1  terminou/terminar\n",
       "\n",
       "[402 rows x 2 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexeme_cluster_results.loc[lexeme_cluster_results.cluster<0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e739cec2",
   "metadata": {},
   "source": [
    "### Match back to user_fingerprint df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1c844320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2709, 5634)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_fingerprint_B = pd.read_csv(\"../data/user_fingerprint_B.csv\")\n",
    "user_fingerprint_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c88089af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>max_history_seen</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>learning_speed</th>\n",
       "      <th>lexeme_0_seen</th>\n",
       "      <th>lexeme_1_seen</th>\n",
       "      <th>lexeme_2_seen</th>\n",
       "      <th>lexeme_3_seen</th>\n",
       "      <th>lexeme_4_seen</th>\n",
       "      <th>lexeme_5_seen</th>\n",
       "      <th>...</th>\n",
       "      <th>lexeme_2805</th>\n",
       "      <th>lexeme_2806</th>\n",
       "      <th>lexeme_2807</th>\n",
       "      <th>lexeme_2808</th>\n",
       "      <th>lexeme_2809</th>\n",
       "      <th>lexeme_2810</th>\n",
       "      <th>lexeme_2811</th>\n",
       "      <th>lexeme_2812</th>\n",
       "      <th>lexeme_2813</th>\n",
       "      <th>lexeme_2814</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u:0X2</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>13.563969</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u:0b</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3.629236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u:0xw</td>\n",
       "      <td>20</td>\n",
       "      <td>108</td>\n",
       "      <td>2.633296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u:1EH</td>\n",
       "      <td>14</td>\n",
       "      <td>52</td>\n",
       "      <td>2.580331</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u:1gx</td>\n",
       "      <td>15</td>\n",
       "      <td>71</td>\n",
       "      <td>7.584663</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5634 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id  max_history_seen  vocab_size  learning_speed  lexeme_0_seen  \\\n",
       "0   u:0X2                15           9       13.563969           15.0   \n",
       "1    u:0b                12          12        3.629236            0.0   \n",
       "2   u:0xw                20         108        2.633296            0.0   \n",
       "3   u:1EH                14          52        2.580331           12.0   \n",
       "4   u:1gx                15          71        7.584663           15.0   \n",
       "\n",
       "   lexeme_1_seen  lexeme_2_seen  lexeme_3_seen  lexeme_4_seen  lexeme_5_seen  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            7.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   ...  lexeme_2805  lexeme_2806  lexeme_2807  lexeme_2808  lexeme_2809  \\\n",
       "0  ...          0.0          0.0          0.0          0.0          0.0   \n",
       "1  ...          0.0          0.0          0.0          0.0          0.0   \n",
       "2  ...          0.0          0.0          0.0          0.0          0.0   \n",
       "3  ...          0.0          0.0          0.0          0.0          0.0   \n",
       "4  ...          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   lexeme_2810  lexeme_2811  lexeme_2812  lexeme_2813  lexeme_2814  \n",
       "0          0.0          0.0          0.0          0.0          0.0  \n",
       "1          0.0          0.0          0.0          0.0          0.0  \n",
       "2          0.0          0.0          0.0          0.0          0.0  \n",
       "3          0.0          0.0          0.0          0.0          0.0  \n",
       "4          0.0          0.0          0.0          0.0          0.0  \n",
       "\n",
       "[5 rows x 5634 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_fingerprint_B.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9f0b0381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexeme_id</th>\n",
       "      <th>lexeme_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>57408f89412af98111a2f87c0ab41b22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>8414835cb39e4315146a59fefdd6d1c6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>ecc3feb8e53ce936cef181dd54e7aaca</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>8d28ba0fa188f1847571467189846dda</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>4b3613233b3fede2e3e92ac2ef752bf6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           lexeme_id  lexeme_code\n",
       "64  57408f89412af98111a2f87c0ab41b22            0\n",
       "65  8414835cb39e4315146a59fefdd6d1c6            1\n",
       "66  ecc3feb8e53ce936cef181dd54e7aaca            2\n",
       "67  8d28ba0fa188f1847571467189846dda            3\n",
       "68  4b3613233b3fede2e3e92ac2ef752bf6            4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>word</th>\n",
       "      <th>lexeme_code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexeme_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57408f89412af98111a2f87c0ab41b22</th>\n",
       "      <td>50</td>\n",
       "      <td>tu/tu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8414835cb39e4315146a59fefdd6d1c6</th>\n",
       "      <td>19</td>\n",
       "      <td>tem/ter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecc3feb8e53ce936cef181dd54e7aaca</th>\n",
       "      <td>19</td>\n",
       "      <td>temos/ter</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8d28ba0fa188f1847571467189846dda</th>\n",
       "      <td>51</td>\n",
       "      <td>tua/teu</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4b3613233b3fede2e3e92ac2ef752bf6</th>\n",
       "      <td>69</td>\n",
       "      <td>leão/leão</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  cluster       word  lexeme_code\n",
       "lexeme_id                                                        \n",
       "57408f89412af98111a2f87c0ab41b22       50      tu/tu            0\n",
       "8414835cb39e4315146a59fefdd6d1c6       19    tem/ter            1\n",
       "ecc3feb8e53ce936cef181dd54e7aaca       19  temos/ter            2\n",
       "8d28ba0fa188f1847571467189846dda       51    tua/teu            3\n",
       "4b3613233b3fede2e3e92ac2ef752bf6       69  leão/leão            4"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pt = dataset_a_raw[dataset_a_raw['learning_language'] == 'pt'].copy()\n",
    "df_pt['lexeme_code'], uniques = pd.factorize(df_pt['lexeme_id'])\n",
    "\n",
    "display(df_pt[['lexeme_id', 'lexeme_code']].head())\n",
    "\n",
    "lexeme_codes = df_pt.drop_duplicates(subset=['lexeme_id', 'lexeme_code']).set_index(\"lexeme_id\")['lexeme_code'].to_dict()\n",
    "len(lexeme_codes)\n",
    "\n",
    "lexeme_cluster_results['lexeme_code'] = lexeme_cluster_results.index.map(lexeme_codes)\n",
    "lexeme_cluster_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4f84e5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>max_history_seen</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>learning_speed</th>\n",
       "      <th>lexeme_metric</th>\n",
       "      <th>weighted_value</th>\n",
       "      <th>lexeme_code</th>\n",
       "      <th>metric_type</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u:0X2</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>13.563969</td>\n",
       "      <td>lexeme_0_seen</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>seen</td>\n",
       "      <td>lex_cluster_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u:0b</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3.629236</td>\n",
       "      <td>lexeme_0_seen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>seen</td>\n",
       "      <td>lex_cluster_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u:0xw</td>\n",
       "      <td>20</td>\n",
       "      <td>108</td>\n",
       "      <td>2.633296</td>\n",
       "      <td>lexeme_0_seen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>seen</td>\n",
       "      <td>lex_cluster_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u:1EH</td>\n",
       "      <td>14</td>\n",
       "      <td>52</td>\n",
       "      <td>2.580331</td>\n",
       "      <td>lexeme_0_seen</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>seen</td>\n",
       "      <td>lex_cluster_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u:1gx</td>\n",
       "      <td>15</td>\n",
       "      <td>71</td>\n",
       "      <td>7.584663</td>\n",
       "      <td>lexeme_0_seen</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>seen</td>\n",
       "      <td>lex_cluster_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15251665</th>\n",
       "      <td>u:yT9</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>13.931693</td>\n",
       "      <td>lexeme_2814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2814</td>\n",
       "      <td>ability</td>\n",
       "      <td>lex_cluster_16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15251666</th>\n",
       "      <td>u:yyO</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>2.044361</td>\n",
       "      <td>lexeme_2814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2814</td>\n",
       "      <td>ability</td>\n",
       "      <td>lex_cluster_16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15251667</th>\n",
       "      <td>u:z4x</td>\n",
       "      <td>85</td>\n",
       "      <td>83</td>\n",
       "      <td>2.375734</td>\n",
       "      <td>lexeme_2814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2814</td>\n",
       "      <td>ability</td>\n",
       "      <td>lex_cluster_16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15251668</th>\n",
       "      <td>u:zmi</td>\n",
       "      <td>12</td>\n",
       "      <td>42</td>\n",
       "      <td>8.380809</td>\n",
       "      <td>lexeme_2814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2814</td>\n",
       "      <td>ability</td>\n",
       "      <td>lex_cluster_16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15251669</th>\n",
       "      <td>u:zzU</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>13.669329</td>\n",
       "      <td>lexeme_2814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2814</td>\n",
       "      <td>ability</td>\n",
       "      <td>lex_cluster_16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15251670 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  max_history_seen  vocab_size  learning_speed  lexeme_metric  \\\n",
       "0          u:0X2                15           9       13.563969  lexeme_0_seen   \n",
       "1           u:0b                12          12        3.629236  lexeme_0_seen   \n",
       "2          u:0xw                20         108        2.633296  lexeme_0_seen   \n",
       "3          u:1EH                14          52        2.580331  lexeme_0_seen   \n",
       "4          u:1gx                15          71        7.584663  lexeme_0_seen   \n",
       "...          ...               ...         ...             ...            ...   \n",
       "15251665   u:yT9                20          13       13.931693    lexeme_2814   \n",
       "15251666   u:yyO                21          14        2.044361    lexeme_2814   \n",
       "15251667   u:z4x                85          83        2.375734    lexeme_2814   \n",
       "15251668   u:zmi                12          42        8.380809    lexeme_2814   \n",
       "15251669   u:zzU                 6          10       13.669329    lexeme_2814   \n",
       "\n",
       "          weighted_value  lexeme_code metric_type         cluster  \n",
       "0                   15.0            0        seen  lex_cluster_50  \n",
       "1                    0.0            0        seen  lex_cluster_50  \n",
       "2                    0.0            0        seen  lex_cluster_50  \n",
       "3                   12.0            0        seen  lex_cluster_50  \n",
       "4                   15.0            0        seen  lex_cluster_50  \n",
       "...                  ...          ...         ...             ...  \n",
       "15251665             0.0         2814     ability  lex_cluster_16  \n",
       "15251666             0.0         2814     ability  lex_cluster_16  \n",
       "15251667             0.0         2814     ability  lex_cluster_16  \n",
       "15251668             0.0         2814     ability  lex_cluster_16  \n",
       "15251669             0.0         2814     ability  lex_cluster_16  \n",
       "\n",
       "[15251670 rows x 9 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_fingerprint_B_lex_clustered = user_fingerprint_B.melt(id_vars=[\"user_id\", \"max_history_seen\", \"vocab_size\", \"learning_speed\"], var_name=\"lexeme_metric\", value_name=\"weighted_value\").copy()\n",
    "user_fingerprint_B_lex_clustered = user_fingerprint_B_lex_clustered.assign(\n",
    "    lexeme_code = lambda x: x.lexeme_metric.str.extract(\"lexeme_([0-9]+)\").astype(int),\n",
    "    metric_type = lambda x: x.lexeme_metric.str.replace(\"lexeme_[0-9]+_?\", \"\", regex=True).replace(\"\", \"ability\")\n",
    ").merge(lexeme_cluster_results[['lexeme_code', 'cluster']], how='left', on='lexeme_code')\n",
    "\n",
    "user_fingerprint_B_lex_clustered['cluster'] = user_fingerprint_B_lex_clustered['cluster'].astype(str)\n",
    "user_fingerprint_B_lex_clustered.loc[user_fingerprint_B_lex_clustered.cluster!=\"-1\", 'cluster'] = \"lex_cluster_\" + user_fingerprint_B_lex_clustered.loc[user_fingerprint_B_lex_clustered.cluster!=\"-1\", 'cluster']\n",
    "user_fingerprint_B_lex_clustered.loc[user_fingerprint_B_lex_clustered.cluster==\"-1\", 'cluster'] = \"lexeme_\" + user_fingerprint_B_lex_clustered.loc[user_fingerprint_B_lex_clustered.cluster==\"-1\", 'lexeme_code'].astype(str)\n",
    "\n",
    "user_fingerprint_B_lex_clustered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7bf1a678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metric_type\n",
       "seen       7625835\n",
       "ability    7625835\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_fingerprint_B_lex_clustered.metric_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7e9b9414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>max_history_seen</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>learning_speed</th>\n",
       "      <th>metric_type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>lexeme_code</th>\n",
       "      <th>weighted_value</th>\n",
       "      <th>lexeme_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u:0X2</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>13.563969</td>\n",
       "      <td>ability</td>\n",
       "      <td>lex_cluster_0</td>\n",
       "      <td>{1155, 2567, 1160, 2188, 2191, 402, 1556, 1814...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>lex_cluster_0_ability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u:0X2</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>13.563969</td>\n",
       "      <td>ability</td>\n",
       "      <td>lex_cluster_1</td>\n",
       "      <td>{258, 2055, 777, 1289, 140, 1935, 1947, 2721, ...</td>\n",
       "      <td>0.01285</td>\n",
       "      <td>lex_cluster_1_ability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u:0X2</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>13.563969</td>\n",
       "      <td>ability</td>\n",
       "      <td>lex_cluster_10</td>\n",
       "      <td>{1031, 1799, 1801, 1035, 1420, 1550, 1678, 245...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>lex_cluster_10_ability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u:0X2</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>13.563969</td>\n",
       "      <td>ability</td>\n",
       "      <td>lex_cluster_11</td>\n",
       "      <td>{1570, 1891, 1668, 197, 1990, 2087, 2584, 2762...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>lex_cluster_11_ability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u:0X2</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>13.563969</td>\n",
       "      <td>ability</td>\n",
       "      <td>lex_cluster_12</td>\n",
       "      <td>{898, 1411, 5, 1669, 524, 2473, 43, 1326, 820,...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>lex_cluster_12_ability</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id  max_history_seen  vocab_size  learning_speed metric_type  \\\n",
       "0   u:0X2                15           9       13.563969     ability   \n",
       "1   u:0X2                15           9       13.563969     ability   \n",
       "2   u:0X2                15           9       13.563969     ability   \n",
       "3   u:0X2                15           9       13.563969     ability   \n",
       "4   u:0X2                15           9       13.563969     ability   \n",
       "\n",
       "          cluster                                        lexeme_code  \\\n",
       "0   lex_cluster_0  {1155, 2567, 1160, 2188, 2191, 402, 1556, 1814...   \n",
       "1   lex_cluster_1  {258, 2055, 777, 1289, 140, 1935, 1947, 2721, ...   \n",
       "2  lex_cluster_10  {1031, 1799, 1801, 1035, 1420, 1550, 1678, 245...   \n",
       "3  lex_cluster_11  {1570, 1891, 1668, 197, 1990, 2087, 2584, 2762...   \n",
       "4  lex_cluster_12  {898, 1411, 5, 1669, 524, 2473, 43, 1326, 820,...   \n",
       "\n",
       "   weighted_value           lexeme_metric  \n",
       "0         0.00000   lex_cluster_0_ability  \n",
       "1         0.01285   lex_cluster_1_ability  \n",
       "2         0.00000  lex_cluster_10_ability  \n",
       "3         0.00000  lex_cluster_11_ability  \n",
       "4         0.00000  lex_cluster_12_ability  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_fingerprint_B_lex_clustered = user_fingerprint_B_lex_clustered.groupby([\"user_id\", \"max_history_seen\", \"vocab_size\", \"learning_speed\", \"metric_type\", \"cluster\"]).agg(\n",
    "    {\"lexeme_code\": set,\n",
    "     \"weighted_value\": \"mean\"}\n",
    ").reset_index().assign(\n",
    "    lexeme_metric = lambda x: x.cluster + \"_\" + x.metric_type\n",
    ")\n",
    "\n",
    "user_fingerprint_B_lex_clustered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5b183fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>lexeme_metric</th>\n",
       "      <th>user_id</th>\n",
       "      <th>max_history_seen</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>learning_speed</th>\n",
       "      <th>lex_cluster_0_ability</th>\n",
       "      <th>lex_cluster_0_seen</th>\n",
       "      <th>lex_cluster_10_ability</th>\n",
       "      <th>lex_cluster_10_seen</th>\n",
       "      <th>lex_cluster_11_ability</th>\n",
       "      <th>lex_cluster_11_seen</th>\n",
       "      <th>...</th>\n",
       "      <th>lex_cluster_79_ability</th>\n",
       "      <th>lex_cluster_79_seen</th>\n",
       "      <th>lex_cluster_7_ability</th>\n",
       "      <th>lex_cluster_7_seen</th>\n",
       "      <th>lex_cluster_80_ability</th>\n",
       "      <th>lex_cluster_80_seen</th>\n",
       "      <th>lex_cluster_8_ability</th>\n",
       "      <th>lex_cluster_8_seen</th>\n",
       "      <th>lex_cluster_9_ability</th>\n",
       "      <th>lex_cluster_9_seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u:0X2</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>13.563969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u:0b</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3.629236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026539</td>\n",
       "      <td>1.340426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u:0xw</td>\n",
       "      <td>20</td>\n",
       "      <td>108</td>\n",
       "      <td>2.633296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007076</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010506</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.011766</td>\n",
       "      <td>0.063830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u:1EH</td>\n",
       "      <td>14</td>\n",
       "      <td>52</td>\n",
       "      <td>2.580331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012514</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.008262</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u:1gx</td>\n",
       "      <td>15</td>\n",
       "      <td>71</td>\n",
       "      <td>7.584663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013606</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.016060</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>u:yT9</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>13.931693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005221</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>u:yyO</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>2.044361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.015141</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007238</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>u:z4x</td>\n",
       "      <td>85</td>\n",
       "      <td>83</td>\n",
       "      <td>2.375734</td>\n",
       "      <td>0.023098</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.012643</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.008833</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.016847</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>0.127660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>u:zmi</td>\n",
       "      <td>12</td>\n",
       "      <td>42</td>\n",
       "      <td>8.380809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008015</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005306</td>\n",
       "      <td>0.170213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>u:zzU</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>13.669329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2709 rows × 166 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "lexeme_metric user_id  max_history_seen  vocab_size  learning_speed  \\\n",
       "0               u:0X2                15           9       13.563969   \n",
       "1                u:0b                12          12        3.629236   \n",
       "2               u:0xw                20         108        2.633296   \n",
       "3               u:1EH                14          52        2.580331   \n",
       "4               u:1gx                15          71        7.584663   \n",
       "...               ...               ...         ...             ...   \n",
       "2704            u:yT9                20          13       13.931693   \n",
       "2705            u:yyO                21          14        2.044361   \n",
       "2706            u:z4x                85          83        2.375734   \n",
       "2707            u:zmi                12          42        8.380809   \n",
       "2708            u:zzU                 6          10       13.669329   \n",
       "\n",
       "lexeme_metric  lex_cluster_0_ability  lex_cluster_0_seen  \\\n",
       "0                           0.000000            0.000000   \n",
       "1                           0.000000            0.000000   \n",
       "2                           0.000000            0.000000   \n",
       "3                           0.000000            0.000000   \n",
       "4                           0.000000            0.000000   \n",
       "...                              ...                 ...   \n",
       "2704                        0.000000            0.000000   \n",
       "2705                        0.000000            0.000000   \n",
       "2706                        0.023098            0.090909   \n",
       "2707                        0.000000            0.000000   \n",
       "2708                        0.000000            0.000000   \n",
       "\n",
       "lexeme_metric  lex_cluster_10_ability  lex_cluster_10_seen  \\\n",
       "0                            0.000000                 0.00   \n",
       "1                            0.000000                 0.00   \n",
       "2                            0.000000                 0.00   \n",
       "3                            0.000000                 0.00   \n",
       "4                            0.000000                 0.00   \n",
       "...                               ...                  ...   \n",
       "2704                         0.000000                 0.00   \n",
       "2705                         0.000000                 0.00   \n",
       "2706                         0.012643                 0.15   \n",
       "2707                         0.000000                 0.00   \n",
       "2708                         0.000000                 0.00   \n",
       "\n",
       "lexeme_metric  lex_cluster_11_ability  lex_cluster_11_seen  ...  \\\n",
       "0                            0.000000             0.000000  ...   \n",
       "1                            0.000000             0.000000  ...   \n",
       "2                            0.000000             0.000000  ...   \n",
       "3                            0.000000             0.000000  ...   \n",
       "4                            0.000000             0.000000  ...   \n",
       "...                               ...                  ...  ...   \n",
       "2704                         0.000000             0.000000  ...   \n",
       "2705                         0.015141             0.055556  ...   \n",
       "2706                         0.008833             0.166667  ...   \n",
       "2707                         0.000000             0.000000  ...   \n",
       "2708                         0.000000             0.000000  ...   \n",
       "\n",
       "lexeme_metric  lex_cluster_79_ability  lex_cluster_79_seen  \\\n",
       "0                            0.000000             0.000000   \n",
       "1                            0.000000             0.000000   \n",
       "2                            0.007076             0.073171   \n",
       "3                            0.000000             0.000000   \n",
       "4                            0.000000             0.000000   \n",
       "...                               ...                  ...   \n",
       "2704                         0.000000             0.000000   \n",
       "2705                         0.000000             0.000000   \n",
       "2706                         0.000000             0.000000   \n",
       "2707                         0.000000             0.000000   \n",
       "2708                         0.000000             0.000000   \n",
       "\n",
       "lexeme_metric  lex_cluster_7_ability  lex_cluster_7_seen  \\\n",
       "0                           0.000000            0.000000   \n",
       "1                           0.000000            0.000000   \n",
       "2                           0.000000            0.000000   \n",
       "3                           0.012514            0.041667   \n",
       "4                           0.000000            0.000000   \n",
       "...                              ...                 ...   \n",
       "2704                        0.000000            0.000000   \n",
       "2705                        0.000000            0.000000   \n",
       "2706                        0.000000            0.000000   \n",
       "2707                        0.000000            0.000000   \n",
       "2708                        0.000000            0.000000   \n",
       "\n",
       "lexeme_metric  lex_cluster_80_ability  lex_cluster_80_seen  \\\n",
       "0                            0.000000             0.000000   \n",
       "1                            0.000000             0.000000   \n",
       "2                            0.000000             0.000000   \n",
       "3                            0.008262             0.152174   \n",
       "4                            0.013606             0.130435   \n",
       "...                               ...                  ...   \n",
       "2704                         0.000000             0.000000   \n",
       "2705                         0.000000             0.000000   \n",
       "2706                         0.007621             0.217391   \n",
       "2707                         0.008015             0.043478   \n",
       "2708                         0.000000             0.000000   \n",
       "\n",
       "lexeme_metric  lex_cluster_8_ability  lex_cluster_8_seen  \\\n",
       "0                           0.000000            0.000000   \n",
       "1                           0.000000            0.000000   \n",
       "2                           0.010506            0.151515   \n",
       "3                           0.000000            0.000000   \n",
       "4                           0.016060            0.121212   \n",
       "...                              ...                 ...   \n",
       "2704                        0.005221            0.121212   \n",
       "2705                        0.007238            0.303030   \n",
       "2706                        0.016847            0.787879   \n",
       "2707                        0.000000            0.000000   \n",
       "2708                        0.000000            0.000000   \n",
       "\n",
       "lexeme_metric  lex_cluster_9_ability  lex_cluster_9_seen  \n",
       "0                           0.000000            0.000000  \n",
       "1                           0.026539            1.340426  \n",
       "2                           0.011766            0.063830  \n",
       "3                           0.000000            0.000000  \n",
       "4                           0.000000            0.000000  \n",
       "...                              ...                 ...  \n",
       "2704                        0.000000            0.000000  \n",
       "2705                        0.000000            0.000000  \n",
       "2706                        0.002748            0.127660  \n",
       "2707                        0.005306            0.170213  \n",
       "2708                        0.000000            0.000000  \n",
       "\n",
       "[2709 rows x 166 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_fingerprint_B_lex_cluster_only = user_fingerprint_B_lex_clustered.loc[user_fingerprint_B_lex_clustered.lexeme_metric.str.contains(\"lex_cluster\")\n",
    "                                                                           ].pivot(columns=\"lexeme_metric\", values=\"weighted_value\", index=[\"user_id\", \"max_history_seen\", \"vocab_size\", \"learning_speed\"])\n",
    "user_fingerprint_B_lex_cluster_only.reset_index(inplace=True)\n",
    "user_fingerprint_B_lex_cluster_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a194a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_fingerprint_B_lex_cluster_only.to_csv(\"../data/user_fingerprint_B_lex_clusters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dfc2d8",
   "metadata": {},
   "source": [
    "#### Normalised version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1d61fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select the columns to be formalized. \n",
    "cols_to_scale = user_fingerprint_B_lex_cluster_only.columns.drop(\"user_id\")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = user_fingerprint_B_lex_cluster_only.copy()\n",
    "df_scaled[cols_to_scale] = scaler.fit_transform(user_fingerprint_B_lex_cluster_only[cols_to_scale])\n",
    "\n",
    "df_scaled.to_csv(\"../data/user_fingerprint_B_lex_clusters_scaled.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duohack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
